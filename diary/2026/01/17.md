# 📔 2026年01月17日（土） の日記

---


### 💻 GRPOによるLLM強化学習と報酬設計・評価フロー解説

**時間**: 13:34  
**カテゴリ**: technology
**📝 STT生テキスト**: [詳細を見る](#stt-9dbf57db)

GRPOを用いてLLMを強化学習する手順と、その際の報酬設計・学習設定・評価方法について解説している講義内容。まず、整数回答タスクを題材に、(1)整数回答の正誤に基づく報酬、(2)指定フォーマット（Boxタグ）を守れているかの報酬、という2種類を組み合わせ、正解報酬に大きな重みを置く設計を説明。出力からBox内の整数を抽出する関数や、Boxが欠けた場合のパース戦略など、実装レベルの工夫にも触れる。

次に、TRLのGRPOトレーナーを用いた学習設定として、学習率、Adamパラメータ、T4 GPU前提での勾配累積やロールアウト数（8だが本来は16〜32が望ましい）、最大出力長512トークンなどを紹介。推論コストが学習時間の大半を占めるため、本格運用ではVLLMによる推論高速化が重要になることや、サンプリング温度を1.0と高めにしてロールアウトの多様性を確保する意図を説明している。

学習ログの見方として、8サンプルのロールアウトに対する正解率・フォーマット遵守率の平均と分散、出力長の推移を確認することの重要性を述べ、事前に学習済みモデルの報酬推移を可視化したグラフでは、ステップが進むにつれて移動平均ベースで報酬が緩やかに向上していることを確認している。

評価パートでは、ベースモデルとGRPO後モデルを同じ条件で比較。テストセット本来は約1300サンプルだが、デモではランダムに抽出した約100サンプルで精度を比較したところ、GRPO後モデルの精度がむしろ劣化するケースが見られ、GRPO特有の学習不安定性（出力崩壊や報酬ハッキング）の可能性、正則化・報酬設計・データ設計の難しさに言及。一方で、個別サンプルではベースモデルが計算ミスした問題をGRPO後モデルが正しく解く例も示され、強化学習によりステップを見直す「Wait」や「Now」などのトークンが増える、いわゆる"Aha moment"的な推論スタイルの変化についても解説している。

最後に、本格的な大規模学習に適したフレームワークとして、ByteDanceのOpenRLフレームワーク VolcEngine Reinforcement Learning for LLMs を紹介。GRPO派生アルゴリズムが実装されており、VLLMやFSDPなどと組み合わせて多ノード・多ワーカーでの並列大規模学習や高速推論・メモリ効率化が行える点をメリットとして挙げている。

---
