# 📔 2026年01月17日（土） の日記

---


### 💻 GRPOによるLLM強化学習と報酬設計・評価フロー解説

**時間**: 13:34  
**カテゴリ**: technology
**📝 STT生テキスト**: [詳細を見る](#stt-9dbf57db)

GRPOを用いてLLMを強化学習する手順と、その際の報酬設計・学習設定・評価方法について解説している講義内容。まず、整数回答タスクを題材に、(1)整数回答の正誤に基づく報酬、(2)指定フォーマット（Boxタグ）を守れているかの報酬、という2種類を組み合わせ、正解報酬に大きな重みを置く設計を説明。出力からBox内の整数を抽出する関数や、Boxが欠けた場合のパース戦略など、実装レベルの工夫にも触れる。

次に、TRLのGRPOトレーナーを用いた学習設定として、学習率、Adamパラメータ、T4 GPU前提での勾配累積やロールアウト数（8だが本来は16〜32が望ましい）、最大出力長512トークンなどを紹介。推論コストが学習時間の大半を占めるため、本格運用ではVLLMによる推論高速化が重要になることや、サンプリング温度を1.0と高めにしてロールアウトの多様性を確保する意図を説明している。

学習ログの見方として、8サンプルのロールアウトに対する正解率・フォーマット遵守率の平均と分散、出力長の推移を確認することの重要性を述べ、事前に学習済みモデルの報酬推移を可視化したグラフでは、ステップが進むにつれて移動平均ベースで報酬が緩やかに向上していることを確認している。

評価パートでは、ベースモデルとGRPO後モデルを同じ条件で比較。テストセット本来は約1300サンプルだが、デモではランダムに抽出した約100サンプルで精度を比較したところ、GRPO後モデルの精度がむしろ劣化するケースが見られ、GRPO特有の学習不安定性（出力崩壊や報酬ハッキング）の可能性、正則化・報酬設計・データ設計の難しさに言及。一方で、個別サンプルではベースモデルが計算ミスした問題をGRPO後モデルが正しく解く例も示され、強化学習によりステップを見直す「Wait」や「Now」などのトークンが増える、いわゆる"Aha moment"的な推論スタイルの変化についても解説している。

最後に、本格的な大規模学習に適したフレームワークとして、ByteDanceのOpenRLフレームワーク VolcEngine Reinforcement Learning for LLMs を紹介。GRPO派生アルゴリズムが実装されており、VLLMやFSDPなどと組み合わせて多ノード・多ワーカーでの並列大規模学習や高速推論・メモリ効率化が行える点をメリットとして挙げている。

---


### 📚 RLVR・GRPOと講義案内・課題締切

**時間**: 13:54  
**カテゴリ**: education
**📝 STT生テキスト**: [詳細を見る](#stt-5ee36f14)

PyTorchのFSDPによる並列学習、RLVR手法とGRPOの特徴および問題点、Pass@kという評価指標とベースモデルとRL強化モデルの比較について説明した講義の締め部分の内容。続いて、第6回演習の終了アナウンスと受講生への労いがあり、今後の講義としてエージェントとロボットに関する講義が予定されていること、講義資料の一部は後日公開予定であること、OmniCampus上でアンケート回答と宿題提出を依頼していること、宿題は既に公開済みであり、出席・宿題ともに締切が「一週間後の11時」であることが伝えられている。

**📋 アクションアイテム**:
- [ ] OminiCampusから講義アンケートを提出する
- [ ] OminiCampusから第六回講義の宿題を提出する

---


### 💻 ドメイン特化LLMの手法と金融・コード事例

**時間**: 14:10  
**カテゴリ**: technology
**📝 STT生テキスト**: [詳細を見る](#stt-166bc79d)

講師の小橋が、大規模言語モデル（LLM）のドメイン特化について体系的に解説した講義の前半部分。まずタスク特化モデルからマルチタスクLLMへの歴史的流れを説明し、ネガティブトランスファーや多言語の呪い、人間脳との構造差からマルチタスクの限界が指摘されていることを紹介。その解決策の一つとして「ドメイン特化」を定義し、BioBERTを端緒として金融・法律・コードなどさまざまな分野で特化LLMが登場してきた流れを述べた。

続いて、ドメイン特化で用いられる手法を「外部拡張（RAG・ツール利用など）」「プロンプトクラフティング（タスク依存・事例依存のソフトプロンプト等）」「ファインチューニング」の三つに大別し、それぞれの学習・推論プロセス上の違いを整理。外部知識の取り込み方として明示的知識（通常のRAG）と暗黙的知識（DPRやRAG-フォーマー系で内部表現を高度化する手法）の違いも解説した。

プロンプトクラフティングでは、タスクごとに最適化されたソフトプロンプトを一度求めて使い回す手法と、各入力ごとに別モデルで動的にプロンプトを生成する手法（例：IDPG）を紹介。ファインチューニングでは、基盤モデルの事前学習データのドメイン偏り、専門知識欠如、方針の不一致を補正する目的を説明し、司法試験向けリーガルプロンプティングの例として、条文とチェーン・オブ・ソート型の説明を加えることで専門家に近い推論をさせる応用を示した。

さらに、ドメイン特化モデル開発の進め方として、(1) ターゲットドメインと目的・制約（機密・倫理・ドメイン規制）の明確化、(2) 専門知識データの収集・整理、(3) 目的に合わせた指標や評価データを自作してでも用意する最適化・評価プロセスの重要性を強調。既存ベンチマークに無理に合わせるのではなく、目的に適合した独自ベンチマーク構築を推奨し、ドメイン専門家自身がエンジニアでなくても特化LLM開発に大きく貢献できると述べた。

後半では事例紹介として、まず金融特化LLM（FinBERT、BloombergGPT、InvestLMなど）を取り上げ、ニュースや開示資料などを使った継続事前学習とセンチメント分類、投資助言、そして包括的金融ベンチマークFinBenの構築について説明。金融RAG事例として、ニュースの文脈補完や専門的金融文書理解、数値データを厳密整形したファンダメンタル分析システムなど、目的に応じたデータ設計の違いに言及した。

続いてコードLLM事例として、自然言語とコードの翻訳タスクからスタートしたPLBART・Codex、複数言語対応で開発環境も含めたCode LLM、完全オープンデータとライセンス透明性を重視したStarCoder、長いコードコンテキストや複数ファイル間依存まで扱えるCodeLlama・DeepSeekCoderなどを紹介。コード理解・生成能力の向上と、実用的なプログラミング支援ツールとしての要件の違いを指摘しつつ、ドメイン特化設計の重要性を示している。

---
