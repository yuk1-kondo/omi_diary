# 📝 2026-01-17 のSTT生テキスト

---


## 📝 GRPOによる数理推論モデル学習と評価 - 9dbf57db-6788-408d-ae08-477d1aa0d30e

**記録時間**: 2026-01-17 13:34:18

### STT生テキスト

🎤 SPEAKER_0 [0s - 73s]
GRPOを学習する上で、報酬の定義が 重要になってきます。 では二つの報酬を用います。 は出力した整数の個体が正解か不正解か 獲物を一ゼロのスパーセナリアドで与えます。 一つは、回答を今回ボックスの中に出力するよう していので、そのボックスの中にちゃんと が出力できていかというフォーマットのリワードも与えます。 二つのリワードを組み合わせて、今回は報酬を定義します。 重要なのは正解していかどうかなので、上の報酬の方が 重みが大きくなるように今回設定しています。 今回は用いませんが、他にも出力の長さを 長くならないようにするだったりだとか、報酬 探索を促すような報酬はよく導入されます ら辺は報酬 のシェーピングっいうところで、まあいろいろ試されていので いろんな研究とかを参考するといかなというふ思います。 は回答の抽出をする 関数から定義していきます。 ではボックスで出力された回答 を、出力の中から抽出するということを行っています。

🎤 SPEAKER_0 [77s - 114s]
関数を使ってそ次に 整数の 回答が一致していかどうかっていうものを判定して、それが合っていれば一、間違っていば ゼロのリワードを与えるような関数をここで定義します。 出力に対して ボックスを抽出します。 がグランド抽出と一致しているかどうかで 一致していればリワード一与えて、一致してなければリワードゼロを与えます。 後にフォーマットのrewardを定義します。 は回答の中にボックスが含まれているかどうか 、含まれていれば0。1、含まれてなければ0で募集を与えます。

🎤 SPEAKER_0 [117s - 232s]
回答の出力のフォーマットは、クエンでよく使用されるボックスを使用し ましたが、他には 例えばハッシュタグ の後に 答えを書くであったりだとか、Yl形式でアンサーの の横に答えを書くであったりだとか、アンサータグの中に 答えを入れるなど、いろんなタグの用いられ方がします。 はモデルがどの、どういうデータによって学習されていかにかなり依存する ので、モデルに合わせて定義するのがいかなといううに思い 。今回クエン三のモデルを用いましたが、クエンではPo で出力することがよく 行われています。なのでBoxの出力を採用しました。 に書いてあるのはクエンの 出力の例になりますが、 、問題が与えられた後に まず問題を解くために 与えられた問題の状況ってものを ステップごとに分解するっことを行って ます。で、その後に各ステップに対して問題を解く という形で問題を解いています。 て最後に 答えが十六 ふうに出た後 このモデルは最後にSorthefinalanswerisBoxの中に十六 ってものを、っていう感じでフォーマットして出力しています。 出力されたボックスの中から十六を抽出して グランドチューズと一致するかどうかっていうものを見ています。 はボックスの中の性質を抜き出し て、その整数が一致するかどうかっていうものを見ますが、で フォーマットを守らなかった場合でも答えを注視できるように 例えばボックスがなかった場合は 最後の部分からその直前にある清掃を取ってくる だったりだとか、アンサーに一番近い清掃を取ってくるなど パースの戦略っいうものは ガードレールを設けてやるっていうのが賢い言い方かなっいうふに思います。

🎤 SPEAKER_0 [237s - 302s]
では実際に学習をしていこうというふに思い 。 で学習する上で、TRMの GRPOトレーナーというのを使います。 GRPOのアルゴリズムですが、ラーニングレートを 五の二の、五の十のマイナスを駆除にして、まあ、アダムの パラメーターであったりとか、コプティマイザーの パラメータを指定しています。 で 今回T4のGPUを行うので、メモリが限らている っていうので、えっとグラデデンターキュミュレーションを使って使っています。 で、あとCRPO 中で重要になってくるのがアドバンテージファンクションを推定するときのロールのアウト 数になりますが、今回八で設定しています。 ですがもし計算リソースに余裕があるのであれば この八、分散がかなり大きいので、理想は十六以上 六であったりだとか、三十二で設定するのが理想かなというふに思います。 で、出力のマックスを五百十二に設定しています。

🎤 SPEAKER_0 [305s - 394s]
で、一般後はよくVLMが用いられますが、今回は環境の 上使わなでやっています。 GRPOは普通のSFTとか 違って、学習の途中で実際にGeeのロールアウト インファレンスして、それを基に 損失関数 を定義しているので 推論のコストが学習の時間の中のかなりの 割合を占めます。 なのでここでVLL、これは推論を高速保する ですが、これを使うというは重要になてきます。 ですがここでは一旦なしで行います。 サンプリングパラメーターですが、テンペラ茶は一点零を定義しています。 クエン酸の推奨のテンペラ茶はだいた とかがオフィシャルから推奨されていますが、実 、GRPOのロールアウトの、今回であば八個のサンプルの中は 多様であることが、まあ、重要ですので、高めのテンペラ帳を設定することが まあ、多々あります。なので今回は一点零を指定しています。 はもちろん零点七のOffialで 推奨されているパラメーターで問題ないというふに思います。 エンドオブトーン、エンドオブセンテンストークンと などをここで定義していますが、今回クエン酸のベースモデル は ドブステイトのトークンがあまり出ないので、マックスのトークン出し てしまことがありますが、それは気にしなでくだい を実行します。

🎤 SPEAKER_0 [399s - 406s]
帽子を適して学習を実行します。 で学習が実行されています。

🎤 SPEAKER_0 [449s - 471s]
今回トータルのステップは五十ステップで学習を行います。 GRPをする場合は だたい一点一ビリオンから十四ビリオンとかのサイズであれば 多くて三百ステップとかで、それ以上やるとかなり計算 コストが高くなるので、 百ステップ、二百ステップがよく取られるステップ数かなというふ思います。

🎤 SPEAKER_0 [476s - 585s]
はい、こんな感じで GRPOトレーナーを実行すると、こんな感じで ステップであったり、トレーニングのロスであったり、リワードみたいなのがずらっと出てきます。 で 重要なのが、まあ今回二つのリワードを提起し ました。 目が、整数の 回答が一致していかどうか、正解してかどうかのリワードと フォーマットを守っていかどうかのリワードの二つがあり 。で、このリワードの平均ですね、今回であれば八個 ロールアウトしていので、その八個のサンプルの中で いくつ正解したかっていう割合、今回まず 一つ目のステップでは、零点一二五なので 八個のロールアウトなどから一つのサンプルしか正解しなかったということになります。 隣には、その八個のロールアウトの分散が書かれ ています。 で、フォーマットに関しては ですね、フォーマットのリワードと平均とそれの 分散っいうが書かれています。なのでこの四つのリワードに関する こう、このカラムをまあ見るこが重要かなっいうふに思います。 は よく見られるのが 全体の利用の平均もそうですが、出力調ですね。 はEOが出ないので 百順位のMAX出ていますが、例えばInsectのモデルから GRPOするのであれば、ここが 一般に今学習すると 出力長が長くなていくっていうふに言れていので、ここがまあ長くなていてるか どうかっていうのを見るのも大事かなっいうふ思います 学習が三十分かかって時間かかるので、一旦この 演習の時間内では 実行せずに、あらかじめ学習させておいたモデルのチェックポイント 見込んで、この後の評価を行います。

🎤 SPEAKER_0 [588s - 657s]
で、あらかじめ学習され、させた モデルの時のえっ、リワードの変化 を可視化しています。 横軸が 強化学習のステップ数、で縦軸がフォーマットと、あと 正解したかどうかのリワードの合計、まあ、一点零足す一点一の合計、一点一マック でリワードが書かれています。青の線が ステップごとのリワードの変化 で、オレンジの線が、この青のリワードの線は ま、今回毎回八サンプルしかサンプリングしなくて、かなり分散 が大きいので 五、ステップで移動平均を取ったものをオレンジで表しています。 オレンジの線を見ると 数が 五から五十に進むにつれて 全体的に 少しずつ上がってる 傾向が 見て取れるかなっいうふに思います。なので今回の まあ零点六ビリオンのまあローラの簡単な設定ですが 訓練のドメインに関しては、ちゃんとリワードが上がってるってのは確認できる かなとふ思います。

🎤 SPEAKER_0 [661s - 734s]
今この訓練した モデルとそれを訓練する前のベースモデルで実際に評価して 性能がどういうふに変わったかっていうのを見てみようというふに思います。 はベースモデルの評価から行いたいと思います。 先ほど学習では ドブセンテストックが出なかったから、そのまま五百十二のマックスの トークンまで出力していましたが、ここでは Boxが出力されたら ストップするような ストップするように設定しています。 でまあそうですね、ボックスの中に 空じゃない何かしらの文字が入ってい場合は を検知して ここで定義して、ストップするっことをします。 ベースモデルですので、元のアンスロスのクエン酸の零点六ミリ モデルを読み込みます。 後、本来であれば テストセット千三百サンプルで 推論するべきなんですけども、今回時間ないので サンプルをシール四十二で ランダムにセレクトしてきて、それを用いて評価を行います。

🎤 SPEAKER_0 [737s - 808s]
、モデル読み込んでインファレンスのモードにして で、各サンプルに対してプロンプトとアンサー、でプロンプトから モデルから推論して、その推論の結果 が このアンサーと一致していかどうかってのを見て正解していれば Collect間違っていばインコレクトっ形で実行します。 を実行すると推論が実行されます。 あらかじめ実行した結果ですが まずここにこういう感じで 問題が与えられます。 問題が与えられた後に 改良でインストラクションが与えられています。 で、ちゃんと最後ボックスの中に出力してくださいというようインストラクションが与えられます。 で、まあ この後にモデルが実際に解き始めて、まあステップを分解して で、各ステップに対して計算していこうみたいな感じでモデルは推移 をしています。で、最後にFinalAnsw がBoxの中で出力されている感じになっています。 で、まあこんな感じで 各サンプルの 出力、まあどういうリーズニングをしていのかってのが見るこができます。

🎤 SPEAKER_0 [811s - 825s]
。これはベースモデル実行してみてくだい。 後、GRPO した後のモデルの評価を行います。で、今回 学習はあらかじめ学習しておいたモデルで行います。

🎤 SPEAKER_0 [830s - 854s]
あらかじめ学習させたモデル読み込んで それをインファレンスモードに持っていって 同じように推論します。 がまあその出力になっています。 も同様に問題が与えられて その後に フリーズリーズ・ステップ・バイ・ップ Boxの中に出力してくだいってのはインストラクション与えられて モデルがリーズニングを開始します。

🎤 SPEAKER_0 [857s - 1054s]
もあらかじめ実行してい ますので、まあ実行してみたい方は実行してみてください。 で、先ほど 私の方で今百サンプル いうのを指して、それの精度っいうのをここに書いてみました。 ですけども で見て分かるとおり、GRP用語のモデルがまあ精度が 劣化していのが分かるかなというふに思います。 に関しては百サンプルなので、少しサンプル数が評価データのサンプル数 が少ないので、もう少しサンプル数を増やす必要はありますが まあ過学習した可能性があるかなっいうのもあります。 GRPOはものすごく学習が不安定 ですので 出力の崩壊であったりだとか 報酬のハッキング とか、そういたものを防ぐための適切な正則化のパラメーターであったりだとか まあ報酬の設計 どの複数の報酬があった場合に、どの報酬を 重要視するのか あるいはデータもちろん重要になってきます。 なので今回のこの百サンプルのテストセット では、まあ、少しGRPO5のモデルが性能が悪化するっいう結果になりま た。実際に出力を見てみますと この問題が与えられて 上がベースモデル 下がGRPR五のモデルの出力になています。 サンプルはフェイスモデルが不正解で、GRPRをすると 展開することができたサンプルになっています。 で、ベースモデルは ここの ゴールドジュエルの 価格の計算で 二千ドルかける五分の四を、まあ、千六百ドルのところを八百ドル いうふに答えてしまっています。 で GRPO5のモデルは プライスオブゴールドのところで五分の四かける二千っ 六百っ形で、まあ計算ミスせずに適切に答え、適切に計算して 結果、ベースモデル は三千二百、誤ったところをGRPをしたモデルは四千八百で正解 してるっていうような サンプルが、まあ、見るこができます。 で、まあ一般にGRPOを 行うと ディプシークの論文で言われた言葉ですが、AfMoment いうものが観察できるというふに言われています。 ArMomentっいうのは、ベースモデルであまり 出てこないようなトークン、例えばウェイトとか そういうようなトークンが、リーズニング、アールエルした後の リーズニングモデルでよく見られるっいうようこになる と言われています。 、WAITっていうと、それまで解いていた数学のステップが間違ってる かもしれないっ言って、そこで一旦立ち止まって、もう一度それを検算するやったり とか、他のやり方を試してみるとか、元の問題文にもう一回立ち戻って みるとか、そういう数学とかそういう推論する上での スキルっていうものを獲得しているみたいなことが言われています。 なのでそういうサンプルがあるかどうかっいうのを 実際に推論させたサンプルから見てみると良いのかなっいうふに思います。 ある単語としては、ここに書いてるオルタニエイティグリ とか 、あるいは

🎤 SPEAKER_0 [1057s - 1070s]
そうですね、Nowとかがよく出てき ね、Nowとかはそこで今からどういうもの いう計算をして、どういうステップを踏でいくのかみたいな プランニングみたいなこを知る時は、なおとかがよく出てくるかなというう思います

🎤 SPEAKER_1 [1074s - 1074s]
は

🎤 SPEAKER_0 [1076s - 1078s]
そうですねウェイト Now

🎤 SPEAKER_0 [1086s - 1093s]
とか そうですね がよく見られるかなっ

🎤 SPEAKER_0 [1098s - 1108s]
そうですね、Wait出てますね。まあ、Wait ここら辺っ出てますね、はい。 こういう まあRLするとよく出てくる単語みたいなのはあります。

🎤 SPEAKER_0 [1113s - 1120s]
で、最後に LLMの強化学習、GRPOなどの学習する上でのフレームワークの紹介をしよう

🎤 SPEAKER_1 [1120s - 1121s]
と思います。

🎤 SPEAKER_0 [1121s - 1168s]
はノートブック、ノートティフォーのGPUであるっいう都合上 、簡単な学習の設定で行いましたが、まあ本格的に 学習を行いたい場合は、このバール VolquenEngineReIforcementLearningforLMSっいうものがおすすめです。 はバイトダンスから公開されている オープンソースのアブストラクションのフレームワークになています。 GRPOの派生のアルゴリズムはいろいろあるんですけども 、それがまあVに実装されているので のアルゴリズムを試してみたい方にも使いやすいかなっいうふに思います。 Vの使うメリットですけども、この つ書いてある 例

🎤 SPEAKER_1 [1169s - 1169s]
、VLM、FSDPっいうのが組み合わされています。

🎤 SPEAKER_0 [1169s - 1211s]
令和 複数のノードであったりワーカーを実行するときに タスクを分割して、でまあそれを最後統合するみたいな処理をしています。 並列にまあ大規模な学習をする上では重要なツールになってきます。 VLMは 推論する際の高速化をする、メモリ管理をするライブラリにな ていて、GRPOは説明 したとおり、アドバンテージ関数の推定のところで 多くの推論を行って、その推論のコストっいうのが 学習時間の中で、大きな割合を 占めます。 ので、この推論の高速化するっことは、GRPOの学習


---



## 📝 RLVR・GRPOと講義案内・課題締切 - 5ee36f14-495b-4c73-afff-e79ae6043e28

**記録時間**: 2026-01-17 13:54:34

### STT生テキスト

🎤 SPEAKER_0 [0s - 25s]
を高速化する上で、まあ非常に重要になてきます。 はPYTorchのFSDPっいうのも実装されていて 、これはまあデータパラレルに加えて重み 購買、オプティマイザーの状態とかもGPUを 十分間で分割するっていうような並列化のツールになってい ます。これがまあ綺麗に実装されているので、VARっていうのは 研究のコミュニティではまあ大きく支持されていかなというふに思います。

🎤 SPEAKER_0 [29s - 75s]
で、最後にRLVRの議論について簡単に紹介したいと思い ます。 最後にまあ整数の出力を出して、それがまあ 答えと一致するかどうか、検証可能かっていうような アウトカム、一番最後に出てくるリワード を合ってるかどうかっていうようなスパースなリワードで 学習するっいうような、まあRLVRの手法を扱いました。 の一つが、GRPOですが GRPOは計算コストが高い、報酬がスパースとかそういう問題以外に ベースモデルが解ける問題っいうものを引き出していだけで 、もともとのモデルの能力を引き出しているだで、新しい推論能力などは

🎤 SPEAKER_1 [75s - 76s]
発見していいっいうような仮説であっり、実験的な結果っいうのが

🎤 SPEAKER_0 [76s - 84s]
報告されています。 能力を測る上で、よく使われる

🎤 SPEAKER_1 [84s - 84s]
メトリックにぱさっとkっていうメトリックがあります。

🎤 SPEAKER_0 [84s - 135s]
ぱさっとkっいうメトリックは、k回推論させて 一つでも警戒すれば そのサンプルに関しては正解したっいうふに見なします。 kを大きくすると ベースモデルの精度、pathatkが GRPOなどの強化学習をしたモデルよりも パサット系が高くなるっいうような現象が報告されています。 パサットワンはGRPをしたモデルが高いんですけども Passat二百五十六とかPassat千二十四にすると ベースモデルのが高いっことが言われています。 、詳しくは、まあここに書いてあるプレプリ、なんとこ に書いてある論文に詳しく書かれていので、見てみる 興味のある方は見てみていただけるといいかなというふ思い 研究のコミュニティでは、まあ今年かなりよく議論されたテーマになている うに思います。

🎤 SPEAKER_0 [139s - 142s]
第六回の演習をこれで終わりたいと思います。

🎤 SPEAKER_0 [154s - 157s]
受講生の皆様、ご受講お疲れ様でした

🎤 SPEAKER_2 [158s - 196s]
は小林浩司と畠山 公私、松谷公私によるドメイン特化についての 講義でした は坂本講師、松島講師 今井講師によるエージェントとロボットについての講義を予定 ておます。 講義資料について現在一部公開していない講義資料が あるのですが、それについては後日公開 公開しますので、今しばらお待ちくだい。 とOminiCampusから

🎤 SPEAKER_1 [196s - 196s]
アンケートと宿題の提出をお願いしております。

🎤 SPEAKER_2 [197s - 210s]
の宿題は既に公開済みです。 出席、宿題共に締め切りは一週間後の 11時です。それでは 本日の講義は終了です。お疲れ様でした


---



## 📝 ドメイン特化LLMの手法と金融・コード事例 - 166bc79d-a5c6-4b00-a26e-1c0e42fd4a46

**記録時間**: 2026-01-17 14:10:10

### STT生テキスト

🎤 SPEAKER_0 [0s - 36s]
特化の講義を始めさせていただきます。 。前半の講師を務めます小橋と申します。よろしくお願いいたします。 私の経歴はこちらのようになっております。ご興味のある方はぜ 資料をご覧いただければ幸いです。 前半の目次となります。 。まずドメイン特化とは何かについて説明した後に その用いられる手法ですね、まあ基本的なもの を紹介させていただきます。その上で前半では 金融と行動 の事例をそれぞれ紹介いたします。

🎤 SPEAKER_0 [40s - 383s]
では そのドメイン特化の前提となる知識 からご説明をさせていただきます。 、LLM なんですが、もしかしたら受講されてい皆さんの 何名かは、そのエレレムが出てきてからその聞い データサイエンス だったりディープラーニング 学習に の触れ始めたという方もいらっしゃるかなと思うんですが、 そのNMが出てくる前というのはそもそも そのモデルはそのタスクごとに 学習させる 言ってしまえばタスク特化ですね。まあタスク特化 で学習させるのがまあそもそも一般的でした に対してLLMだったり、トランスフォーマー というものが出てきたことによって、一つのモデルで複数の を行わせるという手法が確立しました。 、こちら小さいですが右下にそのGoogleが開発したT5 というモデルの例を紹介しているんですが まあ、こちらではその入出力を全てテキストに 統一することによって、まあいろいろなタスク を一つのモデルが使えるようにしたというものになっております。 に加えてあのインコンテキストランディング だったり、まあ、さまざまな手法が開発されることによって 現在ではプロンプトで指示を変えることによっ よって、一つのモデルで複数のタスクを実行するこが 可能になりました。 でそのトランスフォーマーの構造 にその多種多様なタスクだったり、ドメインの情報を詰め込むのは 難しいのではないかという指摘も存在します。 、その一つがネガティブトランスファーと呼ばれる 現象でして、まあそのある学習 であるタスクについてまあ性能を上げようとすると の別のタスクの性能が下がってしまうと いった現象のことをネガティブトランスファーと言います。 、こちら一つのモデルに覚えさせるタスクの 量が多ければ多いほど、こういう現象が必然的に起こってしまうと いうことになります。で、類似した現象としまして の多言語モデルですね、まあいろんな言語が使えるモデルにおいて あまりデータに含まれていない その低リソース言語と呼ばれる言語を学習させる 結果、その英語のようなその高リソース言語の 性能が低下してしまうという、まあ多言語の呪い という現象も指摘されています。 別の観点からの指摘もありまして、そのTransformerという そのアーキテクチャーが、まあ、人間の脳とは異なって その機能ごとにモジュール化するという 性質を持っていないために まあ、本質的にマルチタスクに向いていないのではという、まあ、認知 科学の観点からの指摘も、ま、存在しています ような問題に対する解決策の一つがドメイン特化 となります。 で、本講義で採用するドメイン特化の定義ですが ま、特定のドメインの文脈データに基づいて 汎用的なレールもカスタマイズし、ドメイン固有の知識 を獲得したり、ドメインの目的に応じて最適化される一 一方で、ドメイン固有の制約によって規制されるプロセス といたします。で、このドメイン特化のまあ一つの端 走りとされている 研究がそのバイオバートと呼ばれるものでして、これ まあこちらはそのバートというトランスフォーマーの一つのモデルに対して その医学論文データをまあ継続事前学習 させた後にファインチューニングすることによって その医学 や医療に関するタスクの精度が 劇的に向上するということが示されました に後続する形で、まあ、金融だったり法律 まあ、高度といた多様なドメインで、まあ特化型のLLMが 開発されるようになりました ドメイン特化 で解決する問題っいうのは、まあ、必ずしも 他の手法では解決できないかというと、そうではなくて まあ、例えばそのリーズニングモデル だったり、Moeと呼ばれるアーキテクチャ などによる解決策も存在します。 ます。それらに対するドメイン特化の優位性について 説明いたしますと、一つは速度やコスト面 ですね。リーズニングモデルやMod と、どうしても学習や推論でコストがかかることが多いと に対してドメイン特化モデルの場合は 学習だったり、まあ特に推論ですね、まあ推論における個性 を下げることができるというメリットがあります で、二つ目はネガティブトランスファーに対して そのリーズニングやMoeはその ネガティブトランスファーの影響を緩和する効果は まあ、あるですが、一つのモデルでマルチタスク を行わせるということに関しては、まあ、本質的には変わっていない ですが、それに対してドメイン 特化は、まあ、言ってしまえば扱うタスクの 数を絞り込むといったアプローチになりますので まあ、より直接的にこの問題に対処していると言えます。

🎤 SPEAKER_0 [390s - 395s]
では次にドメイン特化で用いられてる 手法の紹介をいたします。

🎤 SPEAKER_0 [399s - 430s]
手法のまあ大きな三分類ですね。まあまず最初に そのラグやツールユースといったその外部の 情報やツールを利用する外部拡張 で、二つ目がプロンプトクラフティングです 。まあこちら、まあいわゆるプロンプトエンジニアリングも含むんですが それ以外にも直接そのモデルに数値を入力するといった アプローチも含まれます で、そして最後ファインチューニング となります。

🎤 SPEAKER_0 [434s - 514s]
その学習だったり推論のプロセス に照らして違いを説明しますと まずドメインに関わる外部知識を 活用するこを前提としているのが、まあ、左の外部拡張と 右のファインチューニング となります。 で、外部拡張の方はその推論時に 外部知識だったりツールにアクセスして LNMの入力に加えたり するというのが、外部拡張ですね に対して、そのモデルの学習時に外部知識を 利用して、まあ、自己学習を行って それによって出来上がった目的特化型のLLMに タスクを入力して出力を得るという違いになります。 で真ん中のプロンプトクラフティングは プロンプトですね。プロンプトの中に外部 が含まれる場合はあるんですが、 の直接外部知識のデータベースを作るというよりは そのプロンプトの 中にその特定のドメインの性能が上がるような 形の調整をしまして 、タスクと共に入力し、まあ、より良い出力を得る というアプローチとなります。

🎤 SPEAKER_0 [517s - 601s]
外部拡張 と、プロンプトクラフティング、あとファインチューニングの例を、まあ、それぞれ一つずつ 紹介いたします。 、ラグの方はまあ皆さんイメージは っなと思うので、まあツールユースに関しての例をここでは紹介しますと 例えば、まあ、ある種の計算 が発生すると、まあ分かっているような ドメインのタスクの場合 その、まあ無数に計算が存在するのでは て、ある程度パターンが決まっていのであれば もうプログラムをもう直接用意しておいて で、その入力に対して計算に必要な数字を LLMが抽出した上で、プログラムに投げて 計算してもらった方が、まあ確実に 正しい計算結果が得られるということが言えるので まあ、こういう使い方が一つ考えられます。 、二つ目がプロンプトクラフティングの例ですが 、こちら数値の例として この左下のこのセンテンスとのが通常の そのテキストによるプロンプトですね。 がまあある種数値化されてモデルに 入力されるんですが、それと同時にその 最初から数値ですね 数値にされている

🎤 SPEAKER_0 [604s - 800s]
ー入力 を、その、出力が良くなるように、ま、この右側の 最適化範ですね、この炎がついていところが、まあ最適化 に関係する処理なんですが この出力が良くなるように、その まあ調整をしまして より良いこのベクトル を変えて、それを入力に使うようにすると それによって全体的な性能を高める いったものとなります ファインチューニングですが 、こちらはそのどういうことかと言ますと ま、ある特定のドメインで その、まあ、どういうタスクが来るかは、まあ、はっきり言ったら分からない けど、その、このドメインでは、ま、こういうタ があるだろうっていう、まあ、一連のタスクですね。ここの真ん中ら辺に書いてある まあその機械翻訳だっり、情報抽出 だったり、まあそのリーズニングですね こういたいくつかのタスクについて、そのファインチューニングで事前に 学習しておくと、その このタスクの周辺にあるまあ未知のタスク に関しても性能が上がると いうことを期待している アプローチとなっております。 では それぞれ順に、えっと、詳細 の、その、基本的な部分は すでに過去の講義で扱っていると思うので 過去の講義では触れていないであろう 細かい部分について補足的な説明をさせていただきます。 最初がその知識拡張のその明示的な知識と暗黙的な知識 識という区別について説明します。 で、明示的な知識に関しては、その第二回、第二の 外部環境の活用で説明さ ていただいたような、通常のラグのような アプローチとなります。でそれに対して 暗黙的知識とは何かというと、その 通常のラグの限界ですね。例えばその検索時にも 用いるベクトル表現が、必ずしもタスクに適した類似度 になっていないといった問題だったり はその通常の入力に 検索して持ってきた情報を追加してインプットするので そのLLMに入力するテキストがどうしても長くなってしまうと 、まあこれによる問題がありますと。 らを克服するために、そのLLMの内部表現で その、暗黙的にそのドメインに関する知識 に対する理解度を高めると だ、明示的知識は具体的な知識なんですけど、暗黙的 知識はそのドメインに関する抽象的な知識ですね。 を埋め込んで、そのドメインにおける そのインプットの理解の精度を高めると いうアプローチになります。

🎤 SPEAKER_0 [804s - 1113s]
、アンモック的知識の例を、まあ、紹介させていただきます。 で、こちらはちょっとその学習も込み のアプローチなんですが ユーザーの入力を処理する エンコーダーと、あとはそのラグの結果検索して得られた ドキュメントを処理するエンコーダーを、それぞれ別に学習すると それぞれ別々で用意して、もう学習してしまうと いったアプローチになります。そのデンスパッセージリトリーバルと 言いますが、これによってまあ単純にその 外部知識の処理っいうのは、まあ 別途行うので、ま、それに特化した形で より精度高く処理できるといった、ま、メリットが あります。 まあ独自のアーキテクチャなので、まあ新たに 学習、モデルを学習する必要が、まあ、ありますと いうことになります で、もう一つ目的知識の例を紹介しますと、R AGと呼ばれるものでして これは何かと言ますと、その通常のラグは そのユーザーが入力した、まあ、クエリ インプットと、検索して持ってきたドキュメントを そのくっつけてLAでも入力するんですけど、 、通常のLLMはこのような形の入力を を想定していない、その事前学習で その学習したデータにはそういう形のものがそんなに 多く含まれているわけではないので LLMがその のラグで行いたいことの意図をうまく組めずに 適切に処理できないっいうそのセマンティックギャップ というリスクがあります。 に対してこの手法では そのrツーフォーマー、スクエアフォーマーと呼ばれる そのクエリとドキュメントを LLMが理解しやすいような形に変換するための 小さなモデルというのを別に用意しまして そのデータ、入力を変換した上で LLMに入力するという形で ラグの精度を上げる いったものになります。 では次に、そのプロンプトクラフティングに 関して、まあ、補足的な説明をいたします。 プロンプトクラフティングなんですが、そのタスク 依存型のプロンプトチューニングと事例依存型のプロンプトチューニング の二つに分けるこができます上の方は そのタスク あるタスクにおいて共通のプロンプトを与えるという前提で まあ、それを最適化するというものに対して、事例の場合は 個々のインプットに合わせて、その毎回毎回 適切なプロンプトを動的に生成する というものになります。 で、タスク依存型は、まあ一度その最適化されたプロンプトが得られれば その後は通常のLLMの推論と同じように扱えるんですが 自転依存型の場合は、その毎回毎回動的に 生成する必要があるので、まあその専用のモデルを別途用意する 必要が出てきます。 で、まずタスク依存型の例なんですが 、こちらはまあ、プロンプトチューニング という手法でして その複数のタスクをまあまとめて のそうですね チューニングするんですが、そのモデルごとに まあ、そのプロンプト、あー、すみませんタスクごとにプロンプトを変え てソフトプロンプトチューニング をすると、そのプロンプトの調整ですね をすることで その本格的なファインチューニングをしなくても、その 十一ビリオンクラスのモデルであれば そのそれぞれのタスクに対する性能が劇的に上がると いった研究になります。 に対してその事例ごとに そのプロンプトを動的に生成する手法としては そのIP、IDGPといた手法がありまして そのもともとのモデルに加えて その二層のニューラルネットワークを元にした その固有のプロンプトを 出力するためのモデル を追加で学習しまして ここのまあちょっと赤の部分ですね。ここにまあ 挿入すると 毎回挿入することによってそのプロンプト のクオリティを高めると アプローチになっております

🎤 SPEAKER_0 [1117s - 1352s]
で、最後そのファインチューニングに関する本 補足的な説明ですが、ま、こちらあのー、基礎編の 第六回でファインチューニングに関する会議を 説明させていただいてるんですが、 、そもそもFineTuningというのは、まあ、ドメイン特化を前提として提案されてきてる 手法ですので、基本的にはまあそのそちらのファインチューニング回を まあ、参照いただくのが一番いいのかなと思っております。 具体的にそのファインチューニングにおい て何を解決しようとしいるかという話なんですが、そう そのファインチューニング前の基盤モデルですね それの学習、その事前学習で行われているデー データは、まずは そのドメインに関してそのバランス は考慮されていないので、まあそのドメインデータが不均一 であると。なので ある得点タスクは得意だけど、こういうタスクは苦手だよねとか いうドメインは得意だけど、こういうドメイン苦手だよねっいうのがどうしても発生してしまう 。あとはその専門知識の欠如。 、これは一般にあるハルシネーションが起こりやすいっていう話 と、まあほぼ近いかなと思っております。 で、あとはその事前学習においても ま、ある程度はそのLLMに期待する そのことっていうのは、まあ方針が、まあ、ありますけど ま、多くの場合、その特定のドメインに関する性能は高めたいっ 言った場合と、事前学習時に想定されている方針っいうものは、まあ一 てないことが多いと ですので、ま、このような 問題を解決するために行うのがまあファインチューニング となります。 、あとはその基礎編第七回で行った強化学習 でも、まあ、同じような効果が期待できる ということなので、ぜひその第七回の教科学習回も基礎 編受講者の方はぜひ再度ご確認いだけるといいのかなと 思います。 ではそのファインチューニングの応用例 について一つ紹介させていただきますと、 そのリーガルプロンプティングというものでして こちら、まあ、シンプルな手法ではあるんですけど、まあ、日本の司法試験 のま、その日分類ですね。そのはいえの タスクに関して まあ、そのまず問題と それに対する回答 はその判断コンテナで条文 を、まあ、与えるのに加えて その判断根拠なる条文をどのように 解釈して結論をるのかということの説明ですね。これを チェーンオブソートの形で与えることによって その専門家が実際に行っている判断と同様の論理プロセスを得れる に行わせるということを狙いにしております なのでやったことしては複雑ではなくて、その まあ、ある種のリーズニング としてその説明、エクスプラネーションを追加していと いう形になります で、ここのエクスプラネーションどうやって準備するんだっいうところなんですけど、 論文で行われているやり方としては、論、条文 から、その この今回の判断に関係性が強い 一文を抜き出したり、もしくはそのGPTスリーを使っ て合成データを生成したりすることによって 準備しております。

🎤 SPEAKER_0 [1356s - 1771s]
まで紹介したようにいろいろな 手法があるんですが 、実際にどういう形でその手法の 採用を決めればいのかというところに関しては その、まあ、どのようなドメインであっても、ま、こうすれば確実に効果を発揮 する という、その普遍的な手法があるかと言われると まあ、今のところはまあ存在するとはまあ言えないかなと思っております。 なのでその実際に自分で開発しようとなった場合には まあ、いくつかのポイントを押さえて、まあ方針を決める 必要が出てきます。まず一つは定義ですね。 対象となるドメイン これは開発しようという人はもう最初から決まってると思んですが それだけではなくて、あとはその開発する目的ですね 同じドメインでもどういうタスクをやらせたいのか どういう形での性能向上を目指すのかというところは明確にしないと あまりドメイン特化を行うメリットは出てこないかなと 思います。 はその守るべき制約、後ほど説明しますけど そのこのドメインで何かをする場合は こういう制約を守らなければいけなというのは、多くの場合 存在するので、ま、この三つを押さえて まあ、開発方針を決める必要がありますと で二つ目は拡張ですね。そのドメインに関する専門知識を 収集整理する という必要性があります。で、三つ目は最適化ですね。 何に対して最適化するのか というところを、まあ、客観的に評価する ためにスコアを決めるだったり その実践的な振る舞いを何かしらの方で最適化すると こが求められます。で、その上で最適化 されたモデルを、まあ、評価すると で、モデルの性能を確認するといったことが 求められます。 で、特にこの定義に関しては て重要ですので、より掘り下げて説明したいと思いますが 例えば、その高度な情報抽出をしたい といった目的なのか テキストの生成だったり要約をしたいだったりとか あと予測ですね、専門家の 知見を生かして何か予測したり何かをレコメンドするといった ことだったりとか は対話型エージェント だっりそのエキスパートシステムですね。なんかその 弁護士に代わって何かしらアドバイスするといったようなもの はその児童行動の生成とか あとコードの分析だったり、どういたことをやらせたのか 。まあ、こういう形でその目的を具体化して いかないと、モデルの開発でなかなか結果が出ない ということが言えます。 で、次に制約なんですが、その 機密性の高い情報は使わなきゃいけないという、まあ、状況は 結構あるかなと思ってまして まあ、こういうもの、その一般的なLMだとそもそも 機密性の高い情報は 入出力だっり学習で使えないと まあ言えるんですが、ま、それを扱えるようにするために まあ、その極力っいうか かなり高い角度で いうものを出力しないように調整するということがまあ求められます 。で二つ目はその倫理的な制約ですね。 特定の地域やコミュニティにおいて、まあ、こういう 挙動しちゃいけないというものは まあ多くの場合存在しますので、まあそういう調整だったり それに対して、まあドメイン固有の規制ですね。まあ金融だっ たり法、あの、その法律にドメインだと そのルールで決まっているからやってはいけないということがありますので そこをちゃんと遵守するような形で 出力されるようにする必要があると いった問題があります。 定義以外のことに関しても触れますと、 まあ、拡張に関しては、その、どの手法を 採用するにしても必要だというところがポイントでして プロンプトクラフティングの場合だと、必ずしもそのデータベースは 要とされないんですが、それでもその 最適化したり評価の段階では データというものは必要となってくるので 必ず用意する必要がありますと。で、データがないと始まらないので のどのような場合でもまずはデータを用意するというところは 欠かさないで行う必要があります で、次に最適化なんですが 、自分の目的とその既存の 何かしらのツールだったりデータがぴったり合うのであれば 当然用意されていものを使うのが望ましいですが、 、そうじゃない時、そうじゃない時に 妥協して既存のものを使うということは 個人的にはんまり望ましくないなと思ってまして 、あくまで目的にぴったり合わせたものを用意する で、ぴったり合わせるために、まあ自力で そういうデータだっり評価ツールを用意するということが 重要になってくると私は考えております で、最後評価に関してもまあ同様でして、 、そのまあ、既存のベンチマークが あれば、まあ当然それを使うのが望ましいですが 合わない場合は、まあ、惜しまずに独自のベンチマークも作成しましょうと いうことが言えるかなと思います。 定義から評価までのどの手続きにおいても 対象となるドメインやタスクへの理解が 肝心になります。 ドメイン特化の私が強調したいポイントと しては、そのエンジニアではない ても そのドメインの専門家である だってそのドメインに対して詳しいっていうのがすごい強みになると思ってまして 専門家であったら当たり前の視点っていうものが エンジニアだと知らなかったり気付かなかったりする ので、 その専門家であるということを強みにして まあ、ぜひなんかそういうことを目指してい方は のドメイン特化のモデルの開発に取り組んでいただけるといいのかなと 思っております

🎤 SPEAKER_0 [1777s - 1790s]
はここから事例紹介に入っていきます。まず金融を得るについて ご紹介いたします

🎤 SPEAKER_1 [1790s - 1790s]
、金融特化LLMはなかフィンLLMと呼ばれたりするんですが、

🎤 SPEAKER_0 [1791s - 1853s]
そのデータソースですね、その金融に関するニュースだったり その分析レポートだっり、そのプレスリリースだったり といったまあ、さまざまなデータソース もありますし、その行いたいタスクもその市場の その反応予測だったり、そのテキスト分類 その質疑応答要約等々 ま、さまざまなタスクがありますと を通して開発したアプリケーションもいろいろあると ことが言われています。 で、あの、ここからはちょっとその字 系列に沿って、そのいろいろな事例を紹介していきたいんですが、 、ま、まず初期のま、取り組みの一つとしてFINBERT と呼ばれるものがありまして、その こちらはその提示されたデータ、まあ 重いテキストデータですね、テキストデータ、例えば

🎤 SPEAKER_1 [1853s - 1853s]


🎤 SPEAKER_0 [1853s - 1971s]
なんかあるニュースが発表されました。それに対する市場の反応はどうなん ろうっていうような話ですね。それを目的として その継続事前学習とファインチューニングを行ったというものになります。 で継続事前学習ではロイターのニュース記事 から金融関係のものを抜き出して 学習させました。 上でファインチューニングでは その金融入試に対するポジティブ、ネガティブ、中立 の三つ、三分類ですね。これをまあ専門家によっ てデータ作成しまして 学生されたというものが存在しまして、それを使っていのが一つ で、もう一つも、まあ、類似のものなんですが その金融ニュースに対して、こちら連続的な推ですね、マイナス一から +1まで の値が付与されているデータ、この二つを使ってファインチューニング しましたと。で、これによって性能の改善がしましたという 研究になります で、今のは厳密言とLLMではなくて 分類に特化した ものなんですが、実 本格的なまあフィンLLMとしてはブルームバーグ GPTというものがありまして、 の目的は金融ドメインにおけるApple 圧倒的な専門性と、一般ドメインにおける汎用能力 を両立させるということが目的となっており 金融データ と一般データを両方その膨大な 量を用意して、その五百億パラメータ のモデルを自伝学習から 行っています。 で、金融データ としては、まあ、多くはウェブからクローリングしたもの だったりニュース

🎤 SPEAKER_1 [1971s - 1971s]
とか、まあその企業の開示書類だっりプレスリリースといったもの

🎤 SPEAKER_0 [1971s - 2000s]
を使ってまして、で 、ある程度粗いですよね。どうしてもこれだけの量を集めるためには、そんなに そのクオリティに拘れないっいうのはありますが、ま、それでも まあある程度まあ専門的な知見に基づいてまあ絞り込んだデータ を収集して学習させることで 金融タスクでは大幅に性能を伸ばし その汎用的なタスクでも

🎤 SPEAKER_1 [2000s - 2001s]
まあ、同程度のモデルに匹敵する性能を出したと

🎤 SPEAKER_0 [2001s - 2021s]
いうことになっております で、目的をちょっとずらしまして、その投資を目的とした モデルとして、そのインベストLMといったものが 存在しまして、こちら投資に関する有用なアドバイス

🎤 SPEAKER_1 [2021s - 2021s]
の提供を目的としております。

🎤 SPEAKER_0 [2022s - 2049s]
で、こちらはまあ、LoRAですね。先ほどはもう受演学 から行ってますけど、こっちはLoRAで、LoRAで 行っているのでデータはそこまでたくん用意しておらず、 とにかく質の高いデータを 少数、まあ少数と言っもま結構な数ですが 用意しまして、こちら二 その専門家による評価と、既存の金融タスクで

🎤 SPEAKER_1 [2049s - 2050s]
高い性能を示しております。

🎤 SPEAKER_0 [2052s - 2162s]
ような形でいろいろモデルが開発されてきた んですが、その金融に関する包括的な評価を するためのベンチマークがあった方がいいだろうということで まあ、そのフィン ペンと呼ばれる、まあ 、そのベンチマークですね。まあ、こちら八項目、情報抽出 、テキスト分析、質疑応答、テキスト生成、リスク管理、予測 意思決定。で、スペイン語対応なって、まあ低リソース言語への対応 ですね。まあこちらを評価するベンチマークが作成されまし た。 で、他にも そのラグですね、ラグを使うことで性能を上げようという アプローチもありまして、 、こちらは まあその金融感情分析、まあ先ほどの市場予測 ですねの制度向上をするときに その金融ニュースとかってその文脈 いちいち説明しないで、専門的な情報、企業名だったり その現象だったりが、もう端的に表現されることが多いので 、なかなかそのLLMは理解ができない というところを、そのラグでその文脈情報を保管するという形で 性能向上を測っているものだったり 、あとはその 金融文書って専門性が高いものがあるので それに対する まあ、理解を高めるためにそのラグで外部情報を持ってくると いうアプローチ はこの一番下ですが その数値ですね、ファンダメンタルズデータに対する分析システムを構築 するために、そのデータ

🎤 SPEAKER_0 [2167s - 2183s]
を、まあ、しっかりその数値を特に きちんと揃えることによって その効率的に、通常のラグよりも精度高く その検索ができるように、ま、していると

🎤 SPEAKER_1 [2183s - 2184s]
たものがあります。なので同じラグを

🎤 SPEAKER_0 [2184s - 2198s]
使う場合でも目的に応じてその 用意するべきデータだったり、その重点的に処理するべき 前処理だっりが変わってくるといったところがポイントかなと 思っております。

🎤 SPEAKER_0 [2204s - 2265s]
で、次コードLLMの事例を紹介いたします。 コードLLMなんですが その二つの視点が あるかなと思ってまして 一つはそのコードに対する理解や生成能力 が向上した。まあ、そのLLMとして そのコード、プログラミングのコードですね を、なんかその英語、日本語 、中国語、プログラミング言語みたいな感じで、その言語の一つとして捉えて に対する理解や生成能力が上がってるのかなっいうところを 評価する っいう文脈と、もう一つがプログラミングの ツールとして見た場合、そのユーザーが自然言語を 利用してこういうことを作ってくれっ の指示をした時にちゃんと実行可能なコードが 自動生成されるかどうか というところを 注目した場合

🎤 SPEAKER_1 [2267s - 2267s]
で

🎤 SPEAKER_0 [2268s - 2290s]
前者に比べると後者は明確にその満たすべき条件が 決まってまして、その技術的な要件が高い ので、まあ、特にそのオープン モデルですね、は初期のものとか は、まあ、その上の観点で研究されていものが多い

🎤 SPEAKER_1 [2291s - 2291s]
のかなと思っております。

🎤 SPEAKER_0 [2292s - 2406s]
で、まずその、まさにその初期の モデルから紹介していますが、そのPMTファイブ と呼ばれるものですが、こちらはその医師の翻訳タスク としてそのモデルを学習しています。 こちらこの図の右側が、ま、プログラミングのソースコードですね で、左側がそのドックストリングという、その プログラムのその関数 まあそのなんか一つのコマンドの単位ですね、そのコマンド 短いプログラムだと思ってもらっていんですが、 、に対する説明が日本語で されてい、あ、日本語ですいません。あの自然言語で されているものを入力した時に その説明に沿った その実際のコードを出力すると はプログラミング言語と自然言語の翻訳 機械翻訳のような形で まあ、その学習するといったものになっております。 で、それに対して その二千二十一年、そのコーデックスですね 今のあのオープンAIから出てコーデックスのその流れの ものですね、なんですが、ま、やってることは基本一緒なん ですが、ここではその機能的にちゃんと動く Pythonの関数を生成するということを目的として おります。 時にGitHubから膨大な ストリングとコードの、まあ、組ですね、を収集して で、もともとある 事前学習のGPTスリーモデルに対して、その あそのドックストリングからコードを生成するという

🎤 SPEAKER_1 [2407s - 2407s]
タスクをファインチューニングで学習しますと。

🎤 SPEAKER_0 [2407s - 2432s]
上でその高度の正しさですね、正しく動くということを評価 するために、ヒューマンイバルと呼ばれるそのプログラミングの問題を 作成しています。 プラスパス@系と呼ばれる、そのコードを 警戒 生成して で、ちゃんと問題を正解できるものを

🎤 SPEAKER_1 [2432s - 2442s]
つ以上出力できるかという指標を提案しています。 で、次にコード

🎤 SPEAKER_0 [2442s - 2448s]
と呼ばれるモデルなんですが、こちらはさらにもう一歩進みまして その機能的に正しい行動を生成して

🎤 SPEAKER_1 [2448s - 2481s]
プログラマーを支援するっていう ところまでを視野に入れたものとなております。 で、ここではそのこれは過去のモデルの多くはPython を扱ってたんですが、こちら二十三種類の言語まで、まあ確定 しまして、その開発環境、プログラマーが実際にプログラムを 作る 書くための開発環境まで含めてその公開しています。 で、大きく三つのタスクを想定しましてで まずはそのLLMにコードを書いてとお願いすると

🎤 SPEAKER_1 [2487s - 2497s]
上で、その行動を 別のプログラミング言語に翻訳して 先ほどその自然言語と

🎤 SPEAKER_0 [2497s - 2500s]
プログラミング言語の翻訳でしたが、こちらはあるプログラミング言語から

🎤 SPEAKER_1 [2500s - 2520s]
別のプログラミング言語への翻訳ですね。まあそういうタスク と、あとはコードを一行ずつ説明してという、そのコード説明 のためのタスク、まあプログラマーシーンを想定しているので まあそういうタスク をまあ想定してモデルが作られています

🎤 SPEAKER_0 [2522s - 2526s]
、多言語に対応したベンチマークとして、新たにHumayPaddexと

🎤 SPEAKER_1 [2526s - 2529s]


🎤 SPEAKER_0 [2529s - 2529s]
ものを公開しております。

🎤 SPEAKER_0 [2533s - 2543s]
失礼しました 別の取り組みとしまして そのスターコーダーと言われる、その透明性と責任を重視した

🎤 SPEAKER_1 [2544s - 2563s]
高性能な行動用のLLMを提供する いった取り組みもありまして、こちらは その大規模なデータセットを構築した上で もう事前学習からもう全てオープン なデータで学習していると

🎤 SPEAKER_0 [2564s - 2575s]
たところが特徴となります。 はそのGitHubでちゃんと許諾されているレポジトリーから 構成されているTheStack と呼ばれるデータセットですね、こちらを構築しますと

🎤 SPEAKER_1 [2576s - 2583s]
上での百五十五億パラメーターのベースモデルを

🎤 SPEAKER_0 [2583s - 2583s]
一丁トークンで事前学習した後に

🎤 SPEAKER_1 [2584s - 2592s]
そのPythonデータでファインチューニング ライセンスもちゃんと省利用できる

🎤 SPEAKER_0 [2592s - 2592s]


🎤 SPEAKER_1 [2593s - 2600s]
ものを付与した上で、もう学習データの どうやって選ばれたのか、どのように評価したのか、あとCOツーの排出量に至るまでを 公開しております

🎤 SPEAKER_0 [2606s - 2610s]
で、まあこのようなスターコーダーのような取り組み と関連して

🎤 SPEAKER_1 [2612s - 2626s]
ー、まあ、これまでそのChatGPTだっり、まあ、クロードだったり、まあジェミニ といたそのクローズドなモデルが高いコーディング性能を してい中で、そのオープンなモデルでも

🎤 SPEAKER_0 [2626s - 2627s]
高い性能を発揮するものが

🎤 SPEAKER_1 [2627s - 2634s]
その二千二十三年以降

🎤 SPEAKER_0 [2634s - 2640s]
公開されていきます。で、一つはまあ高ドラマですね。 は発表された時に結構話題になりましたが

🎤 SPEAKER_1 [2640s - 2705s]
まあ、特徴としてはそのこれまでのモデルに比べると、長い入力 に対応するといっ ところが挙げられて、これまではその関数という短い コードの出力を想定したんですが、 より長いものが使えるようになったと で、ディープシークコーダーはさらに その扱えるそのコードの量を増やしまして もう一つのコードじゃなくてその複数のコードですね にその何かを開発するときにはコードが一つだけということは まあ、なかなかなくって、複数の行動を組み合わせて一つの システムを開発することになるんですが、その ファイル同士の依存関係まで含めて学習しましたよと いうものとなります。 で、構造的には、Googleでの 公開モデルですが、まあ、こちらはコード保管だ ったり高度生成だったり自然言語生成といたさまざまなタスクに対応させますと


---

