# 📝 2026-01-17 のSTT生テキスト

---


## 📝 GRPOによる数理推論モデル学習と評価 - 9dbf57db-6788-408d-ae08-477d1aa0d30e

**記録時間**: 2026-01-17 13:34:18

### STT生テキスト

🎤 SPEAKER_0 [0s - 73s]
GRPOを学習する上で、報酬の定義が 重要になってきます。 では二つの報酬を用います。 は出力した整数の個体が正解か不正解か 獲物を一ゼロのスパーセナリアドで与えます。 一つは、回答を今回ボックスの中に出力するよう していので、そのボックスの中にちゃんと が出力できていかというフォーマットのリワードも与えます。 二つのリワードを組み合わせて、今回は報酬を定義します。 重要なのは正解していかどうかなので、上の報酬の方が 重みが大きくなるように今回設定しています。 今回は用いませんが、他にも出力の長さを 長くならないようにするだったりだとか、報酬 探索を促すような報酬はよく導入されます ら辺は報酬 のシェーピングっいうところで、まあいろいろ試されていので いろんな研究とかを参考するといかなというふ思います。 は回答の抽出をする 関数から定義していきます。 ではボックスで出力された回答 を、出力の中から抽出するということを行っています。

🎤 SPEAKER_0 [77s - 114s]
関数を使ってそ次に 整数の 回答が一致していかどうかっていうものを判定して、それが合っていれば一、間違っていば ゼロのリワードを与えるような関数をここで定義します。 出力に対して ボックスを抽出します。 がグランド抽出と一致しているかどうかで 一致していればリワード一与えて、一致してなければリワードゼロを与えます。 後にフォーマットのrewardを定義します。 は回答の中にボックスが含まれているかどうか 、含まれていれば0。1、含まれてなければ0で募集を与えます。

🎤 SPEAKER_0 [117s - 232s]
回答の出力のフォーマットは、クエンでよく使用されるボックスを使用し ましたが、他には 例えばハッシュタグ の後に 答えを書くであったりだとか、Yl形式でアンサーの の横に答えを書くであったりだとか、アンサータグの中に 答えを入れるなど、いろんなタグの用いられ方がします。 はモデルがどの、どういうデータによって学習されていかにかなり依存する ので、モデルに合わせて定義するのがいかなといううに思い 。今回クエン三のモデルを用いましたが、クエンではPo で出力することがよく 行われています。なのでBoxの出力を採用しました。 に書いてあるのはクエンの 出力の例になりますが、 、問題が与えられた後に まず問題を解くために 与えられた問題の状況ってものを ステップごとに分解するっことを行って ます。で、その後に各ステップに対して問題を解く という形で問題を解いています。 て最後に 答えが十六 ふうに出た後 このモデルは最後にSorthefinalanswerisBoxの中に十六 ってものを、っていう感じでフォーマットして出力しています。 出力されたボックスの中から十六を抽出して グランドチューズと一致するかどうかっていうものを見ています。 はボックスの中の性質を抜き出し て、その整数が一致するかどうかっていうものを見ますが、で フォーマットを守らなかった場合でも答えを注視できるように 例えばボックスがなかった場合は 最後の部分からその直前にある清掃を取ってくる だったりだとか、アンサーに一番近い清掃を取ってくるなど パースの戦略っいうものは ガードレールを設けてやるっていうのが賢い言い方かなっいうふに思います。

🎤 SPEAKER_0 [237s - 302s]
では実際に学習をしていこうというふに思い 。 で学習する上で、TRMの GRPOトレーナーというのを使います。 GRPOのアルゴリズムですが、ラーニングレートを 五の二の、五の十のマイナスを駆除にして、まあ、アダムの パラメーターであったりとか、コプティマイザーの パラメータを指定しています。 で 今回T4のGPUを行うので、メモリが限らている っていうので、えっとグラデデンターキュミュレーションを使って使っています。 で、あとCRPO 中で重要になってくるのがアドバンテージファンクションを推定するときのロールのアウト 数になりますが、今回八で設定しています。 ですがもし計算リソースに余裕があるのであれば この八、分散がかなり大きいので、理想は十六以上 六であったりだとか、三十二で設定するのが理想かなというふに思います。 で、出力のマックスを五百十二に設定しています。

🎤 SPEAKER_0 [305s - 394s]
で、一般後はよくVLMが用いられますが、今回は環境の 上使わなでやっています。 GRPOは普通のSFTとか 違って、学習の途中で実際にGeeのロールアウト インファレンスして、それを基に 損失関数 を定義しているので 推論のコストが学習の時間の中のかなりの 割合を占めます。 なのでここでVLL、これは推論を高速保する ですが、これを使うというは重要になてきます。 ですがここでは一旦なしで行います。 サンプリングパラメーターですが、テンペラ茶は一点零を定義しています。 クエン酸の推奨のテンペラ茶はだいた とかがオフィシャルから推奨されていますが、実 、GRPOのロールアウトの、今回であば八個のサンプルの中は 多様であることが、まあ、重要ですので、高めのテンペラ帳を設定することが まあ、多々あります。なので今回は一点零を指定しています。 はもちろん零点七のOffialで 推奨されているパラメーターで問題ないというふに思います。 エンドオブトーン、エンドオブセンテンストークンと などをここで定義していますが、今回クエン酸のベースモデル は ドブステイトのトークンがあまり出ないので、マックスのトークン出し てしまことがありますが、それは気にしなでくだい を実行します。

🎤 SPEAKER_0 [399s - 406s]
帽子を適して学習を実行します。 で学習が実行されています。

🎤 SPEAKER_0 [449s - 471s]
今回トータルのステップは五十ステップで学習を行います。 GRPをする場合は だたい一点一ビリオンから十四ビリオンとかのサイズであれば 多くて三百ステップとかで、それ以上やるとかなり計算 コストが高くなるので、 百ステップ、二百ステップがよく取られるステップ数かなというふ思います。

🎤 SPEAKER_0 [476s - 585s]
はい、こんな感じで GRPOトレーナーを実行すると、こんな感じで ステップであったり、トレーニングのロスであったり、リワードみたいなのがずらっと出てきます。 で 重要なのが、まあ今回二つのリワードを提起し ました。 目が、整数の 回答が一致していかどうか、正解してかどうかのリワードと フォーマットを守っていかどうかのリワードの二つがあり 。で、このリワードの平均ですね、今回であれば八個 ロールアウトしていので、その八個のサンプルの中で いくつ正解したかっていう割合、今回まず 一つ目のステップでは、零点一二五なので 八個のロールアウトなどから一つのサンプルしか正解しなかったということになります。 隣には、その八個のロールアウトの分散が書かれ ています。 で、フォーマットに関しては ですね、フォーマットのリワードと平均とそれの 分散っいうが書かれています。なのでこの四つのリワードに関する こう、このカラムをまあ見るこが重要かなっいうふに思います。 は よく見られるのが 全体の利用の平均もそうですが、出力調ですね。 はEOが出ないので 百順位のMAX出ていますが、例えばInsectのモデルから GRPOするのであれば、ここが 一般に今学習すると 出力長が長くなていくっていうふに言れていので、ここがまあ長くなていてるか どうかっていうのを見るのも大事かなっいうふ思います 学習が三十分かかって時間かかるので、一旦この 演習の時間内では 実行せずに、あらかじめ学習させておいたモデルのチェックポイント 見込んで、この後の評価を行います。

🎤 SPEAKER_0 [588s - 657s]
で、あらかじめ学習され、させた モデルの時のえっ、リワードの変化 を可視化しています。 横軸が 強化学習のステップ数、で縦軸がフォーマットと、あと 正解したかどうかのリワードの合計、まあ、一点零足す一点一の合計、一点一マック でリワードが書かれています。青の線が ステップごとのリワードの変化 で、オレンジの線が、この青のリワードの線は ま、今回毎回八サンプルしかサンプリングしなくて、かなり分散 が大きいので 五、ステップで移動平均を取ったものをオレンジで表しています。 オレンジの線を見ると 数が 五から五十に進むにつれて 全体的に 少しずつ上がってる 傾向が 見て取れるかなっいうふに思います。なので今回の まあ零点六ビリオンのまあローラの簡単な設定ですが 訓練のドメインに関しては、ちゃんとリワードが上がってるってのは確認できる かなとふ思います。

🎤 SPEAKER_0 [661s - 734s]
今この訓練した モデルとそれを訓練する前のベースモデルで実際に評価して 性能がどういうふに変わったかっていうのを見てみようというふに思います。 はベースモデルの評価から行いたいと思います。 先ほど学習では ドブセンテストックが出なかったから、そのまま五百十二のマックスの トークンまで出力していましたが、ここでは Boxが出力されたら ストップするような ストップするように設定しています。 でまあそうですね、ボックスの中に 空じゃない何かしらの文字が入ってい場合は を検知して ここで定義して、ストップするっことをします。 ベースモデルですので、元のアンスロスのクエン酸の零点六ミリ モデルを読み込みます。 後、本来であれば テストセット千三百サンプルで 推論するべきなんですけども、今回時間ないので サンプルをシール四十二で ランダムにセレクトしてきて、それを用いて評価を行います。

🎤 SPEAKER_0 [737s - 808s]
、モデル読み込んでインファレンスのモードにして で、各サンプルに対してプロンプトとアンサー、でプロンプトから モデルから推論して、その推論の結果 が このアンサーと一致していかどうかってのを見て正解していれば Collect間違っていばインコレクトっ形で実行します。 を実行すると推論が実行されます。 あらかじめ実行した結果ですが まずここにこういう感じで 問題が与えられます。 問題が与えられた後に 改良でインストラクションが与えられています。 で、ちゃんと最後ボックスの中に出力してくださいというようインストラクションが与えられます。 で、まあ この後にモデルが実際に解き始めて、まあステップを分解して で、各ステップに対して計算していこうみたいな感じでモデルは推移 をしています。で、最後にFinalAnsw がBoxの中で出力されている感じになっています。 で、まあこんな感じで 各サンプルの 出力、まあどういうリーズニングをしていのかってのが見るこができます。

🎤 SPEAKER_0 [811s - 825s]
。これはベースモデル実行してみてくだい。 後、GRPO した後のモデルの評価を行います。で、今回 学習はあらかじめ学習しておいたモデルで行います。

🎤 SPEAKER_0 [830s - 854s]
あらかじめ学習させたモデル読み込んで それをインファレンスモードに持っていって 同じように推論します。 がまあその出力になっています。 も同様に問題が与えられて その後に フリーズリーズ・ステップ・バイ・ップ Boxの中に出力してくだいってのはインストラクション与えられて モデルがリーズニングを開始します。

🎤 SPEAKER_0 [857s - 1054s]
もあらかじめ実行してい ますので、まあ実行してみたい方は実行してみてください。 で、先ほど 私の方で今百サンプル いうのを指して、それの精度っいうのをここに書いてみました。 ですけども で見て分かるとおり、GRP用語のモデルがまあ精度が 劣化していのが分かるかなというふに思います。 に関しては百サンプルなので、少しサンプル数が評価データのサンプル数 が少ないので、もう少しサンプル数を増やす必要はありますが まあ過学習した可能性があるかなっいうのもあります。 GRPOはものすごく学習が不安定 ですので 出力の崩壊であったりだとか 報酬のハッキング とか、そういたものを防ぐための適切な正則化のパラメーターであったりだとか まあ報酬の設計 どの複数の報酬があった場合に、どの報酬を 重要視するのか あるいはデータもちろん重要になってきます。 なので今回のこの百サンプルのテストセット では、まあ、少しGRPO5のモデルが性能が悪化するっいう結果になりま た。実際に出力を見てみますと この問題が与えられて 上がベースモデル 下がGRPR五のモデルの出力になています。 サンプルはフェイスモデルが不正解で、GRPRをすると 展開することができたサンプルになっています。 で、ベースモデルは ここの ゴールドジュエルの 価格の計算で 二千ドルかける五分の四を、まあ、千六百ドルのところを八百ドル いうふに答えてしまっています。 で GRPO5のモデルは プライスオブゴールドのところで五分の四かける二千っ 六百っ形で、まあ計算ミスせずに適切に答え、適切に計算して 結果、ベースモデル は三千二百、誤ったところをGRPをしたモデルは四千八百で正解 してるっていうような サンプルが、まあ、見るこができます。 で、まあ一般にGRPOを 行うと ディプシークの論文で言われた言葉ですが、AfMoment いうものが観察できるというふに言われています。 ArMomentっいうのは、ベースモデルであまり 出てこないようなトークン、例えばウェイトとか そういうようなトークンが、リーズニング、アールエルした後の リーズニングモデルでよく見られるっいうようこになる と言われています。 、WAITっていうと、それまで解いていた数学のステップが間違ってる かもしれないっ言って、そこで一旦立ち止まって、もう一度それを検算するやったり とか、他のやり方を試してみるとか、元の問題文にもう一回立ち戻って みるとか、そういう数学とかそういう推論する上での スキルっていうものを獲得しているみたいなことが言われています。 なのでそういうサンプルがあるかどうかっいうのを 実際に推論させたサンプルから見てみると良いのかなっいうふに思います。 ある単語としては、ここに書いてるオルタニエイティグリ とか 、あるいは

🎤 SPEAKER_0 [1057s - 1070s]
そうですね、Nowとかがよく出てき ね、Nowとかはそこで今からどういうもの いう計算をして、どういうステップを踏でいくのかみたいな プランニングみたいなこを知る時は、なおとかがよく出てくるかなというう思います

🎤 SPEAKER_1 [1074s - 1074s]
は

🎤 SPEAKER_0 [1076s - 1078s]
そうですねウェイト Now

🎤 SPEAKER_0 [1086s - 1093s]
とか そうですね がよく見られるかなっ

🎤 SPEAKER_0 [1098s - 1108s]
そうですね、Wait出てますね。まあ、Wait ここら辺っ出てますね、はい。 こういう まあRLするとよく出てくる単語みたいなのはあります。

🎤 SPEAKER_0 [1113s - 1120s]
で、最後に LLMの強化学習、GRPOなどの学習する上でのフレームワークの紹介をしよう

🎤 SPEAKER_1 [1120s - 1121s]
と思います。

🎤 SPEAKER_0 [1121s - 1168s]
はノートブック、ノートティフォーのGPUであるっいう都合上 、簡単な学習の設定で行いましたが、まあ本格的に 学習を行いたい場合は、このバール VolquenEngineReIforcementLearningforLMSっいうものがおすすめです。 はバイトダンスから公開されている オープンソースのアブストラクションのフレームワークになています。 GRPOの派生のアルゴリズムはいろいろあるんですけども 、それがまあVに実装されているので のアルゴリズムを試してみたい方にも使いやすいかなっいうふに思います。 Vの使うメリットですけども、この つ書いてある 例

🎤 SPEAKER_1 [1169s - 1169s]
、VLM、FSDPっいうのが組み合わされています。

🎤 SPEAKER_0 [1169s - 1211s]
令和 複数のノードであったりワーカーを実行するときに タスクを分割して、でまあそれを最後統合するみたいな処理をしています。 並列にまあ大規模な学習をする上では重要なツールになってきます。 VLMは 推論する際の高速化をする、メモリ管理をするライブラリにな ていて、GRPOは説明 したとおり、アドバンテージ関数の推定のところで 多くの推論を行って、その推論のコストっいうのが 学習時間の中で、大きな割合を 占めます。 ので、この推論の高速化するっことは、GRPOの学習


---

