{
  "id": "b95ec851-21c0-4273-a739-b74a88075401",
  "created_at": "2026-01-21T10:09:44.016771+00:00",
  "started_at": "2026-01-21T10:10:03.105150+00:00",
  "finished_at": "2026-01-21T10:50:25.955817+00:00",
  "source": "omi",
  "language": "ja",
  "structured": {
    "title": "基盤モデルとフィジカルAI時代のロボティクス",
    "overview": "講義形式で、2023年前後から加速している基盤モデル（LLM・VLM・ロボティックファンデーションモデル）をロボットに応用する潮流が解説された。まず、単一モデルで多様なタスクに対応できるプロンプティング、高スケーリングによる汎化性能、マルチモーダル対応という基盤モデルの特徴が、ロボットの多様なセンサー・アクチュエータと相性が良い点が整理された。\n\n続いて、LLMを用いたロボット動作計画やコード生成（例：手元APIを組み合わせてタスクプランを自動生成するCoder‐styleの研究）、LLMで報酬関数を生成して強化学習に使う手法、CLIPなどの特徴を地図や物体検索に埋め込むマルチモーダル活用（Clip-Fields系）、音声・画像・テキストをAPIとして組み合わせたロボットシステム構築例などが紹介された。\n\nその後、ロボティックトランスフォーマー（RT-1/RT-2）のようなエンドツーエンドのロボット基盤モデルや、Google RobotX/RT-Xのような多機体・多環境データの統合事例を通じて、大規模データ収集の重要性が論じられた。テレオペによる実機データ、世界中の研究機関からフォーマットを揃えて集めるOpenX-Embodiment的プロジェクト、Aloha/Mobile Aloha・ゼロなどの低コストテレオペセットアップ、UMIグローブやGoPro等による人間動作計測など、現実世界データの収集手法が具体的に示された。\n\nさらに、NVIDIA Isaac/OmniverseなどGPUベースの高忠実度シミュレータによるSim2Real、ディフュージョンポリシーやTransformerベース世界モデル（GATO的アプローチ含む）など、多様な学習パラダイムがロボット行動生成に導入されていることが説明された。\n\n応用・産業動向としては、中国・米国を中心に、ヒューマノイドの大規模テレオペセンターや、フィジカルAI・汎用ロボットのスタートアップ（工場・物流・家庭内作業向け1X、Figure AIなど）の台頭、ヒューマノイド価格の低下と量産によるスケール効果が紹介された。また、NVIDIAは人工データ（シミュレーション）、GoogleやMetaはウェブデータやウェアラブル経由の人間データなど、それぞれ「リアルロボットデータ・人工データ・ウェブデータ」の三層をどう組み合わせるかという戦略の違いが整理された。\n\n最後に、リアルタイム性と大規模モデルの性能トレードオフ、データとモデルが一部企業に集中することへの懸念、既存ビジネスを支援するツールとしてデータ取得インセンティブを設計しつつ長期的に自動化率を高めていくロードマップの重要性、そしてモーターやハードウェア設計レベルから基盤モデル前提でロボットを設計し直す必要性が強調された。講義は、19:55再開予定の休憩アナウンスで締めくくられている。",
    "emoji": "🤖",
    "category": "technology",
    "action_items": [],
    "events": []
  },
  "transcript_segments": [
    {
      "id": "729f0cdf-c389-40ea-9da8-1dab82900e59",
      "text": "ノイドを活用した研究みたいなのも増えてきていと いうような流れになています。 で、最初のスライドの方でも 一回しゃべったんですけど、 最初二十三年の初期の方だと 基盤モデルですね。皆さんLLMだとかVLと VLMというものを ロボットで活用しましょうっいうような動きがまず行われてきましたと いう話をできればいかなと思います。",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 2.384185791015625e-07,
      "end": 30.05000023841858,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "a45f48ce-f2da-4489-8b39-e7afcc9004dc",
      "text": "で、ここはもう完全に何回も何回も て、社会専攻のとこあると思いますけど、なんでロボットに対して 基盤モデルを活用するのがいいのかというところで、大きく分けると この三つ、基盤モデルの特徴自体それぞれロボットに結構マッチ してるよねっいう話ができればいかなと思います。一つ目が いろんなタスクで単一のモデルで解けると つ目がモデル計算用としてスケーリングで汎用、汎化性が上がると 。三つ目が言語に限らなんじゃないかみたいなところですね。 目の基盤モデルの特徴っ いうところは、いろんなタスクを単一のモデルで解ける ところ、特にプロンプティング、入力を変更する こで書類の出力を得ましょうみたいな技術が 出てきてますけれども っていうのはロボットごとにじゃあ その場に合わせて、毎回毎回で違うタスクをやらせるとき 違うモデルを作りますかっていうと、我々がロボットに対してやらせたタスクは すごい星の数ほどあるので、それぞれでこの モデルを作っていたら、きりがないわけですよね。 なので、このA11とかV11とかでやられるような、プロンプティングっいう性質 使い方ていうのは、何か 、事前学習の時にたくさデータ必要です 活用する時に学習が必要ないとか、めちゃめちゃコストが下がる という点で、かなりロボットと相性がいいわけです。 二つ目に関しては、データとか モデルとか計算量のスケーリングっいうのが成立っていよねっていうのが一番目特徴でしたけど 、やっぱり先ほど申し上げたようにいろんな環境でいろんな タスクをやらせたいと、いろんなロボットでやらせた ことを考えると、事前学習モデル自体とか、ロボット の活用するときのベースになるようなモデル自体、汎化性がすごい高まっ てほしいわけですよね。このあたり DMとかVLMではかなり特徴的ですけど、ロボット の基盤モデルBLAという非常にランキングアクションモデルでも、こういうスキーがあるんない かと言われてきているので ごとえかなりい性質なんじゃなかというふに言われていま 三つ目は特にこれ大事だと思ってるんですけど、三 、ロボットっていろんなセンサーであるとか、アクセ ターというのを持ってるわけですね。カメラだけではなくて じゃあ点群を撮れます、振動画像を撮れます、点群を撮れます。 マイクがあって、言語 言葉を入出力できますみたいな、他の 触覚センサーありますみたいな状態で いろいろ動き回るわけですけど、これま機械学習だとかなり それぞれのモダリティごとに特徴を多く設計してみたいなことをやってましたが、 モデルというかトランス村以降ですね、かなり方法論として 言語であるとか、画像であるとか、その他の 点群データであるとか共通化されてきていて 、それらを組み合わせてマルチモデラーモデルを作りやすくなていると いうようなふうの期日の発展が起ってますんで、これ 値を活用してロボットの 認識であるとか、制御っいうのを賢くできるんじゃな というので期待されているというところになります。",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 35.74000023841858,
      "end": 232.85000023841857,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "c719e093-04fd-47bb-ae3b-c802588c4f6b",
      "text": "で、まず、じゃあ 基盤モデル自体ですね、LLMをロボットの システムで活用しましょうというのが、歴史的というか二十三年以降の 流れとしても最初に起きたことですけれども、代表的な使われ方をお話しでき ばと思います",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 239.44000023841858,
      "end": 255.20001023841857,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "3ef6c54f-e775-429b-a661-fa651dadc482",
      "text": "は 二十三年の前半であるとか二十二年の後半ぐらいから きたことですけど、この事前学習されたLLMを まずロボットの動作計画に伝えましょう というような研究が行われてきました。 では、皆さんプログラミングする時って プログラム各LLですね、コーディングするLLをマクロドコードとか いろいろありますけど使ってらっしゃる方も多いんじゃなかなと 思いますけど、そういうような LLMのプログラム書けるような能力っいうのが じゃあロボットの制御に活用できるんですかね、どうなんでしょうねっいうのを 試したのが、このIから二十三っていう 国際会議のところですね、に採択さたコーダーのコイルシーティー用な研究です。",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 258.26000023841857,
      "end": 306.0500202384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "1053dd78-d913-40d7-83d7-40d7db183043",
      "text": "まあこれ何やってるかというと 環境、ロボットのカメラで環境を認識するようなAPIと 手先をここからここに動かしますよみたいなコントロールのAPI っていうのを事前に用意しておくと 、ユーザーが か言ってるんで、空のボールの中に ロックを積んでねっていうようなのを自然言語で Lelv与えると 事前に用意されたAPIを組み合わせて プログラム、ロボットを動かすためのプログラム、これ会社で書かれてますけど、プログラムを 作ってくれますよ。でそれを実際動かしてみまたよっていうのが、このデモ です。結構今から見ると大丈夫だよなっいうような感じはし ますけれども、この時は結構コーディングとしてベー使えますよね ところもあって、結構これ意外にロボット ちゃんと正しく認識、それなりにちゃんと正しく認識できればできれば ロボットの動作性、動作性性って 結構できるんだねみたいなことが起動されました。 。ちょっと面白い使い方としては このLLMのコーディングできる能力というのを 、ロボットの動作の学習ですね。 に活用して報酬 関数としてですね こういう動きをしたら行ってあげますよみたいな 強化学習に使うための報酬関数を生成するためにエレベーモ 使えますよっていうような研究をやりまた。右側で今 動画出てますけど、で バランスボールで四脚ロボットが玉取りして前に進んでるんです 、これを まずシミュレーション、シミュレーターの中で 学習するときに、例えば球の上に乗っかってたらプラス 一点ですみたいな。で、四脚、四方の足が全部 弾から始めちゃたらマイナス零点一点です、みたいなのを 実際にこのロボットが学習された様子を見ながら 精緻化していく、報酬活動を精緻化していくのをエレベーやりますよ のをやって、で、それを使ってこのロボットの動作の 学習を許可学習でやります いうような研究があったりします。 まで エレレムをロボットのプライミングであるとか、動作の学習に という話でしたけど、先ほど申し上げたように 基盤モデル自体、資格であるとか言語を組み合わせると いうのがかなりできるようになってきてますので、 いうマルチモーダルモデルを ロボットシステムに活用しましょうねというような研究もよく行われてきています。 代表的なのはこのクリップフィールドっていう 論文で、BLMの中にクリップっいうものが すごい初期ですけど、あると思んですけど、そこへの 特徴用をロボットが地図作る時にですね ね、地図作る時ロボットが 深度画像とかを使って点群を作るわけですけれども、 、そこの点とかに特徴を埋め込んでいくと いうことをやりますとのをやってのは、このFuiteHillという研究です。",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 309.1000002384186,
      "end": 508.1100002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "17c1db53-d57a-42a3-964c-47b0faad9305",
      "text": "けどみたいなこをロボットにテキストで言うと に地図撮ってる時に、食べ物に関係しそうなところキッチンだね か、ここに銀行があったりとか、ここに冷蔵庫あったりみたいなところがあるので 、結構このタスクに関係しそうなポイントで とか、地図、オブジェクトっいうのを取ってくることができるようなります ということを示しているのが システムとして示したのはPonQuickFzっいう研究になります。 。同じようなことは、結構簡単に やろうと思えばできて、これは統計の事例です ですけど、三年前ぐらいにロボカップ アットホームっいう毎年家庭環境内でタスク 発話でから、ロボットに何かやらせたいことを発話して ロボットが動作する、実際その通り動作するっいうようなのをやってる カフェというのは、元々協議会の中のタスクがあるんですけど、それで 統計の中で、学部一、二年生が あLLMであるとか、VLMであるとか、さっき言っ クリップフィルスみたいなのを組み合わせて、ロバストに 発話から動くようなシステムを作ったとか、そういうことがあります。 LLM がプライニングできるっいう能力はかなり重要 で、なおかつ最近の ビジョンランゲージモデルであるとか、音声認識モデルっいうのも、言語を介して、言語 テキスト情報を介して入出力できるようになっいので、 、こういうことをやろうとしたけどものをつかもうとしたけど失敗し 旅損ねたよねみたいなのもテキストで結果が出てくるわけです 。そうすると、失敗をキャッチして それをリカバーするようなブラインドっていうのを自分 で生成することができるよね。だからずっと動き 続けることができるよねみたいなことを、研究室の中でやってた 学生とかもいました",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 513.9400002384186,
      "end": 632.6900002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "6d83c3ca-264b-42ed-8564-861a2592383f",
      "text": "ちょっとだけ我々同じDelay挟みましたけど、こういうふに 結構事前学習されたLLMであるとかVLM は、かなりAPIで活用できるっていう時代にもなっ てますので、それらを組み合わせたような ロボットのシステムを作ってみましたっいうような研究はかなり 増えてきているという状況になっています。 とはいえ、これまでさっき ちらちら、講談ポリシーの時も言ってましたけど 、ロボットの動作自体ですね、プランニングができようなったけれども、ローレベル コントローラーといたりしますけど、じゃあ物体をつかみますよってプランニングが出 、例えば出てきたときに、そこにつかむためにロボットの完成度をどういうふに動かす すのとか、どのぐらいの力でつかめばいいのみたいな ところのローレベルなコントロールまで",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 640.2500002384186,
      "end": 694.0099502384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "bd5336ad-c09b-41ac-8013-f0ce1fae4882",
      "text": "全容易するんじゃなくて、ちゃんとデータから学習することで、もっと柔軟に 動作するんじゃないかみたいなことが議論されて、K てまして、実際にそういう方向に 最近の基盤モデルであるとか、ロボット基盤モデルであるとか 、ディジカルAIの方向が進んできてますというのがあります。 ためには、実際にいろんな環境であるとか、まあ シミュレーションでもいんですけど、 、データを、ロボットが動いたっいうデータが必要になるので。 そのデータをどうやって集めましょうかねというところを中心に 研究がなされてきています。 で、まずモデルの観点から言いますと、 、ここにあるこのRT1とかRT2、ロボディ ファンデーション、ロボティックトランスフォーマーという研究がGoogleからなされているん ですけれども、VLMと同じように ロボ、モデル自体が 視覚と言語の情報からロボットの 行動ですね、関節の角度とか手先の位置みたいな ボットのコードを直接出力するモデルを 大規模モデルとしてN2で学習できるんじゃなかっていう研究が話されて きました。 でRT1とか35で Mで、RT2とか12Bぐらいのパラメーターで、いろんな データをですね 、ロボット遠隔操作であるとかして集めていると そこから学習するということをやっています。このRT1っいうのは 結構、今から考えるとそんなに大きくはない んですけど、テキストの情報とロボットが のカメラの情報からアトラスバーのベースのモデルを学習して ロボットの先の位置であるとかアームの位置であるとか 第一波動作であるとか、つかみます 話しますみたいなウィップハンド動作ってのを 直接出力するっていうようなモデルになって ますと。で、これをやるために、インスタントアート 書いてありますし、あとちょっと紹介しますけど 、たくさんGoogleのオフィスの中にロボットを放ってですね 沿岸をさせることで学習したというのがあります。 いうことができると、これちょっと参考的な情報ですけど、VLM としての学習というのもできるようになって 、こういうここにrootがこうやってこういうふなことが見えてるけど 何をしたら良さそうですかっいうので、まあこの出力として 間接隔離、さっきは間接隔離したけど、こっちはまず いうふにこれをつかんでみたいな、テキストを返すというのも同じよう 仕組みでできますよねというふなのが言われたりしてます。 で、こういうようなロボットのための基盤モデル ロボティックファンデーションモデル、ロボット基盤モデルって言ったりもしますけど、これを 作るためにデータをまず集めるのが大事だよ というのが、ここ一、二年で話されてきていことです。 、LLMであるとかVLMの場合は インターネット上にたくんまずデータが あって、Comokroidであるとかいろんなのでフローリングしたデータセットというのか たくさありますので、それをレバレッジしていくというのがまずやられてたことですけど。 ロボットの場合はインターネット上にじゃあたくさんどうさせたデータがありますか ていうと、そんなことはなくてですね、まだ集めるというところから始まるので、 、まずロボットのデータ形式が統一 しましょう。、いろんなところから集められる仕組みを見ましょう というのが研究としてなされてきています。 きの一番最初のRT1っいう 研究だと、Googleが自分たちで使う たロボットオフィスの環境二十三台ぐらい 解放って一年半ぐらいずっと遠隔離れて集めたというのを やってたんですが、これ自宅動かされてるんけど 、これは結構大変ですし、こんなできるGoogleぐらいだろう っ感じですし、で ロボットだけ使ってますので、じゃあ他のロボットではどうなんいっ いう話になってくるわけです。 なので、その後すぐにですねウェディングマインドと 世界中の研究機関が自分たちが持っているデータ ロボットでのデータというのを出し合って、同じフォーマットに 変換して公開するというプロジェクトをやっていまし rtxっいうような プロジェクトで、データセット自体オープンxmodumentっいう データセットなんですけど、黒線EmodIment っいうのは、いろんな身体ですね、のデータを ドイツ的に公開して学習しましょうというのをやってるプロジェクト、テスト さっきのRTVはRTVのオリジナルの モデルっていうのは 、もともとはタイツのGoogleのロボットで学習 してたものですけど、 らのいろんなロボットの いろんな携帯のロボットでまず事前学習した モデルを作って、それを活用して 使いたいロボットのモデルを作ると いうことをやると、個別のデータで、最初から個別のデータで学習したような 同じモデルよりもeaNodになりますよっていうことが言われています と。つまりいろんな環境であるとか、いろんな身体の ロボットでの事前学習っいうのが効果でありますよねというのが 示されたのが、このrtxデプロ研究になります。 からですね、じゃあ んな環境とかいろんなタスクのデータを集めるのがすごい大事だよ ねということが議論されるようになって、できるだけコスト安 くデータを集めようねというのがやられし",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 697.6400002384186,
      "end": 1054.8601002384187,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "ef880003-1838-48c9-8a80-2ad7ab89cb74",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1054.8601002384187,
      "end": 1055.1800002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "b033decf-4b6d-42c7-8ea1-9042633d5b84",
      "text": "できてたところで、すごく有名な セットアップで言うと、AlohaとかMobileAlohaっいうのが、スタンフォード化 提供されて、結構ミニバーオールみたいにしてですね 、使いたることの後ろにこのコントローラーを作って、同じサイズのコントローラーを て、か交差しましょうみたいなことをやっていたりもしますし 、ロボットを生活保護るために同じ ぐらいの価格をするロボットを使うとちょっとアホらしいんで コントローラーの方をちっちゃくしましょうみたいなゼロっていうような プロジェクトがあったりしまた。これは結構 プリント可能だったりして、我々の研究室で作ったりして、すごく 役立ちますよってことはやられています。 で にこういうデータを作るためには、そもそも手先だけあばいいんじゃなのっいう 話もあって、いろんな 動作をしたのを簡単に集めるために、こういうグローブ型アンド型の デバイスですね、GoProでデータ集め回すみたいな、簡単にスクレームをするよみたいな デバイスも提案されたりしてます。これはYmiっいうような ユニバーサルマニピューションインターフェースっいうものなんですけど、研究が出てきたりして",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1055.1800002384186,
      "end": 1120.0500002384185,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "e51480eb-eeda-421b-80e2-cd4c31947fe2",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1120.3500002384185,
      "end": 1120.7499002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "99d0181b-c9c0-4cc1-830a-7d1502a45c66",
      "text": "ます。データの集め方としても 、世界のデータだけではなくて、シミュレーションを伝えましょうと いうような研究もかなり進んできてます。特に シミュレーターで大阪 作、ロボット動作を学習して、リアルで活用しましょう、シムツーリアルっ言んですけど は、昔から二千十九年ぐらいからずっとやられてたんですが、最近 NVIDIAがこのフィジカルAI領域を 強化しているのもあってで 、GPUで高速にたくさんデータを生成できる すごくリアルなデータをたくさん生成できるような フレームワークというのを積極的に開発して るおかげで、かなり進んできています。 代表的なものがOmiBusticFrameworkだたり、Dyesacceneというものなん ですけれども、 、これはGPUで アクセラレーションができていて、右上側に四脚ロボ がうようにいますけど、たくさんのロボットを 同時並行的に シミュレーション、力学シミュレーションできますよだとか、複雑 な形状での物議計算というのもできますようだ とか、右下の真中のところですね、は もともとGPUっレンダリングに使われますけど、 それを活用して、かなりEISというかこれ と見どっちが見えるか分かないですけど、かなりリアリスティックな 画像のデータを たくん人工的に作れますよというようなフレームワークを提供したり",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1122.1900002384186,
      "end": 1221.4800002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "f7516ee2-6a3b-455f-bf36-137aaa8c949e",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1221.4800002384186,
      "end": 1222.0399002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "ea984d9b-fbf2-408b-8b57-2cabcd823f52",
      "text": "してます。こういうことが起きて きて、その上にですね、もっと学習で使いやすいような 学習のインターフェースみたいなのも開発を進めてきてる、おかげで 最近ちょっと例が古くて四客ロボットにしか動画上げてまい ですけど、最近エックスとか見てるとヒューマノイドがなんか踊ったり なんだボクシングしたり、なんか作業したりしてると思いますけど、 、いろんな環境で ロバストに動くような 方策、制御法則っいうのは学習できますよと いうのが示されてきているわけです。 こんな感じで、データの集め方として、ギエルのデータをたくさん 集めるという方法であったり、シミュレーションで活用しましょうというような方法 でやられてきてました。で、ちょっとモデルの方に行くと さっき基盤モデルがマルチモーダルであるというのがかなりロボット にとっていいよねと、ロゴフィジカルAが進む流位になてる っ話をしまたけど、 その側面を切って 最近で気ままでアーキテクチャ してのランソマーがベース、いろんなモダリティでベースで使われている のでで 、それぞれドメインごとに研究されてた モデルっいうのが、言語も 音声も ロープのコードも同じようなアーキテクチャーで統合し合う くなてるよというのが、かなりこういうロボットの基盤モデルを作る上で推し されてる理由になてるだろうというふうに思ってます で、どういうふうにじゃあ それらの各モダリティのTransformerベースのモデルでの特徴を組み合わせますかっていうような なのが、研究もされていたりもしますし、 学習するときに、じゃあデータを どういうふにに参加しましょうか。に参加したトークンしましょう であるとか、操作に参加せずにいいんじゃなかみたいな話もあったりであるとか、 、学習するときに のLLMとかVLLMもよく得られますけど、 、時点学習として効率よくデータ効率 よく学習するためにどのようにマスクをかけましょうかみたいな 研究っていうのも、これを作るための研究 してかなり出てきてると 状態です。さっきちらっと言ましたけど、画像生成でよく 使われてるディフュージョンモデルっいうのも、このロボットの講堂先生の学生 のところに 活用されてて、上側にディフュージョンポリシー ってディフュージョンモデルを活用したロボットのコード生成するようなモデルの 結果を出してますけど、TRIを中心に、こういうような 柔軟な動きをする、えー、ようなモデル学習方法 の拡散方法とか、他の 画像生成モデルで、試験を活用して研究もされてきています。",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1222.3600002384185,
      "end": 1402.7400002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "7b1e51c3-8f0c-4364-b799-e3871b536e9f",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1404.8300002384185,
      "end": 1405.5499002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "821da11a-9161-468b-a9d7-106498a2449e",
      "text": "で、同じように学習の方法です ね、としても、最初初期は 入力画像と言語で、出力ロボットの行動に 参加したものですみたいな、直接予測する形で 学習というか、モデル作られてましたが ちょっと強化学習っぽくですね、ロボットの 行動に対する価値を出力するモデルですね、を作って Qラーニングしましょうっいうようなモデルが 出てきたりですね。 でちょっといわゆる世界モデルっぽいんですけど、 未来予測ですね、ロボットの未来、行動の ある状態とコード入れたとき、どういう状態になかなっいうような 、これフォワード、フォワードモデルと言って自分で言ってたします 、予測モデルをベースとして、じゃあ実際どのコードを取ればいいんだっけっいうのを推定 ようなモデルの作り方っいうのが作られていたいな か、こんな感じにして、ちょっと前の研究ですけど、GATOっいうのもいろんな モダリティのデータ統一的に扱っては、Wordモデル作りましょうと 、このような研究がなされてきています。さっきちらっと言いまたけど 言ったような、フォワードモデルを作りましょうである とか、ロボットのコードを直接推定しましょうであるとか 共感握手をしましょうみたいなのは、実は同じような 統一的なフレームワークとして書けるよねみたいな研究を 。みんスクテストですけど行われてきて 、シミュレーター作りましょうみたいな研究も行われています",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1406.4299002384187,
      "end": 1499.5599002384185,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "046e8d04-c277-4390-ad24-d01d984d8b85",
      "text": "で、最後ですね、まあ 残り十分ぐらいの時間を使って、最近のフィジカル 領域、AIかけるロボティクスの応用の話を できればいかなと思います。 で、特にここ一年ぐらいAI 、フィジカルAIの領域だと アメリカとか中国を中心にして 、さっきデータがすごい大事になってきたんじゃなしれけど、このリアルのデータをどれだけ 確保しましょうかというところの競争になってきている外面がありますと 例えばですけど、左側は 一番最初にできたところで言うと、中国の 上海のヒュマンドローイングセンターっいうので",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1507.2200002384186,
      "end": 1550.9501002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "04a7c942-48f9-4c02-800c-07d7bc7f547c",
      "text": "の数よりもどんどん増えてるみたいなんですけど、 、上海のビルの中にですね、まあヒューマノイドをたくさん置いて で、遠隔操作でこう後ろに人いますけど、コロンビア環境で人がいますけど 、遠隔操作でリリアルなデータを集めましょうと いうのをやっていたりもしますし、こういうデータシステムを担うような スタートアップっいうのもUSJ中国中心に出てきてますと。 。、まあ右下とかもこの さっきのUMIっいうデバイスを使って、ロボット用のデータを集めるような サービスを行っていところが期待している。",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1554.0000002384186,
      "end": 1590.2600002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "e95d8e7f-0a37-4bea-befe-ca069b719be8",
      "text": "。こういう フィジカルAIであるとか 汎用ロボットのスタートアップというのはかなり出てきてますし て Cobagainoっ会社は、NewsBirkleの有名な ピーター・ビルっ研究者が過ごしたところで、工場とか物流領域での",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1604.5900002384185,
      "end": 1606.6500002384187,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "ea9a83c6-0e6d-4b8c-b43a-d2950814065f",
      "text": "",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1617.5499002384186,
      "end": 1619.2300002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "6ad9d6f3-1c23-44a8-9487-d1fc2f108442",
      "text": "研究開発というのをやっています。 自動化っいうのをやってますし、一番最初に述べたフィジカルインテリジェンスっいう会社 といっ訳されたりしますけど、もう 元Googleの人であるとか、石橋 ことの本当にこの領域を作ったというか 、加速されてきたような研究者がスタート 行って開発を行っているという状態 態になってます。 他にもまあヒマノイドのスタートアップもかなり出てきて ていて、有名なところで言うと 1Xっいう会社ですね。これは 左は 家の中で家事していることですけど、そう",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1619.6000002384185,
      "end": 1660.9600002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "ba0d7820-e9a8-4f8f-acb4-9daabe340282",
      "text": "社でこういうヒュマノイドを作って自動化するような モデルも作るというのをやってるところもありますし、右側のFigureAIっ ところもヒューマンで自分で作って、BMWのこの工場内で 生産技術の一つとして、 自動化をするというのをやってるような 会社もあります。 で、まあ中国の動きもかなり 特徴的で もう最近 たくさんいろんなところでヒマノイドの動画を見ることあるかもしれなですけど、 ヒューマノイドの価格もかなり 安くなってるわけですね。 か数年前、十年前とかだったら、暇の量",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1664.0399002384186,
      "end": 1713.7300002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "c659741a-d8ff-4e07-b2ff-5ff4f711a94d",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1713.7300002384186,
      "end": 1714.1300002384187,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "6b234f72-4648-4a52-b780-222fbeca3c54",
      "text": "一個三千万は安いね、ぐらいの 話をしてたわけですけれども、最近は 筋前で買えるのもって肥満が出てきたりだとか、 、やっぱ量産の効果は大きくてですね、いろんな スタートアップがコーヒーマノイドを開発しては売てると いうような 状態でどんんスケール効果が出てきていというような状況になて ます。 中でたくさんロボットが どんど作られていっていくと、こう データと経験ロボットの経験度がたくさ溜まってくるわけで、 、じゃあいかにしてそれを活用するかというような 議論が ここ一、二年活発に行われているわけでし で、最初にコールという社会がありますよって話 しまたけど、まあそこでもかなり議論されていまして このデータに関するワークショップというがありますと。 中で、結構考え方として 構整理されてるなと思ったのが、このNVのゆかずっいう リサーチャーがいてた話ですけど 、結局ここでは三パターンにロボット の基盤モデルを作るところの時のデータを整理してます と。一つがリアルロボットのデータ、二つ目が人工データ つ目がウェブデータっいう分け方をしてますと 。で、このこれがピラミッドみたいになってて、これの ピラミの三要素を 横断して、データを用意して学習するのが大事だよと言ってるのが きっかけられるか言ってることですけど 、それぞれに性質があるわけ ですね。リアルロボットデータっいうのは、まあある意味 我々が 場で使いたいロボットで、やらせたいタスクのデータがあれ 、データを取ることが、たくさん取ることができれば、それが一番 教師信号としては強い。一方で、やっぱり それをじゃあテレワーク操作で取りますか。全部取ります か、たくさん何万時間も取りますかっていうと そんなことはコストが高すぎて現実的ではないと いうのがあって強制信号としては強いんだけれども データの、ここでアバンダンシーっ言ってますけど、まあ データに対してのコストっていうのは分かりますよね。 で、ウェブデータですね、YouTubeであるとか これは後で言ますけど、一認証視点の動画を撮りましょうみたいな話もあるんですけど。 、こういうウェブに上がってるようなデータというのは、すごくいろいろ んな領域でのいろんな作業をしているデータとしては存在しい かもしれないけれども、じゃあ目の前にあるロボットを動かしましょうとい た時に、じゃあこうどこかの説をこぐらい動かせばこういうふに動きますよねみたいなのは 序盤持ってないわけなので 汎用性は広い カバーしてる領域は広くて、データ、一データ残したり はすごい安いかもしれないけど、あるロボットを動かそうとしたときの挙手 シグナルとしてはそんなに強くない",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1714.1300002384187,
      "end": 1893.9299002384187,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "ce0e5e2c-3ea1-4e8c-b660-603ea9e2a4cd",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 1894.6700002384187,
      "end": 1894.9900002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "7addf32f-cc3b-4559-a2e9-2d35da0d2e07",
      "text": "で、真ん中にある人工データっていうのは ある意味中間的で、あるリアルのデータを、じゃあ リアルトゥーシムっ言ますけど、リアルデータを使ってシミュレーションを作って、そこでいろんな データを再生成します。データオグメンテーション データ拡張っ言ったりもしますけど いうのは、まあ結構 中間的ですね。リアルデータを集めるよりは ちょっとコスト安くできるかな。だけど、で多様なものが取れるかな ってところがあって、まあこれを組み合わせることが大事だよね いうのを言っていたりし ぞれですね、なんか この汎用ロボットのプレーっいろいろいますけど、それぞれ強みを生かして、実際は 研究だとか開発を進めてるなというのはよく分かります というので、最初に述べたフィピン・ティーシュバークレとかGoogleの たちがやってるフィジカルインテリジェンスとかは、このリアルデータを ばしばし取って一万時間とか、この前 たら数万時間取ってるっ言っましたけど 、ここを強めにして作ってますし、 、まあNVIDIAとかはこのシミュレーターめちゃめちゃ強いので、まあここの データのところをレバレッジして進めてますし、 、Googleもジミロボティックスといっジミナイを使ったロボットの 今モデル作ってますけど、ここはこのジェミナーのこの YouTubeだかインターネットのデータ の汎用性っいうのを活用して、ロボットと組み合わせますよと いうのはやったりしています。 、コード前にこちらって書いてるですけど、Metaとかも こういうアプローチを取ってるなというのは 思っていて、人間のデータを ウェアラブルデバイス、最近Metaはウェアラブルデバイス 作ってますけど、ああいうので集めて、ロボットに活用しますよって経験をたり",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 1894.9900002384186,
      "end": 2013.3000002384185,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "89c9596c-fd98-40d5-aa27-5f6157d1ed36",
      "text": "",
      "speaker": "SPEAKER_2",
      "speaker_id": 2,
      "is_user": false,
      "person_id": null,
      "start": 2013.3000002384185,
      "end": 2013.4601002384186,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "6a5db0e4-16fc-4bc2-bc08-718379cc4e0e",
      "text": "してますと。このアプローチ自体結構面白いなというふには思ってまし て、最後一言付け加えると ある意味 さっきいろんなロボットでデータを取ると 動作に対する たけど、ある意味 この現実世界に存在している 一番数の多いロボットって人間じゃないですか。なので 人間のデータをたくさん忘れておいて 、それで動作の時点学習をしておいて 使いたいロボットに活用しましょうと。 いうのをやったりする方法も考えられるんじゃなかなというの 例えばですけど、まあこれ我々の研究室の中で 一人称視点の首掛け が取れるような一人称視点のデバイス使って ロボットを直すような動作をですね データを取っておくと、まあこれ最近のVRMすごいんで 動作認識みたいなのをさせると、まあユニアクチュアートを組み立てて回すみたい っ、順章みたいなのを作れるわけです。まあこれ自体人間の 支援をしてくれてるわけですけど、手順書作って、次なんか 順でやってくとばらせますよとかできるわけですけど いう人間を支援するツールとしてデータが取れるっいうインセンティブを データを取るっいうインセンティブを作ると、結果として そのデータが ロボットに汎用ロボットに自動化につながってるみたいなストーリーだ と、データを 何もビジネスにならないのにデータをたくさん研ぎましょうみたいな ところにならずに、今目の前の問題を解く ためにデータを取るんだけど、それが結果としてること 状態につながっていので、ロングタームにも こいつされているというような、総義を組みやすいかな というふに思っていますので、こういうなんか人間のデータと言ましょうみたいなのは結構 最近、この研究領域としても着目されていアプロチになります。 までデータの話をしてきましたと。 で、最後これまとめですけど 、最初、エルネームで ロボットのシステムを作って、プラギングさせましょうと いうことをやってたけれども、じゃあその動作のAPIっいうのをどうしてくんですか。 みたいな話になって、それだけだと、なんか失敗したりて をしたりだとか、十七語大きいできないからAnuteEndで作る 動作できるようなモデルを作りましょうっいうような議論がされてき 話をしましたけど、そうするとどんどモデルの大きさっいうの 大きくなってきていくわけですね。そういう 数ビリオンとか数十ビリオン程度を VLAで使ってますけど、それを そのぐらいになってくると、じゃあ推論するのに零点一 だとか、一秒とかかかってくるわけです。で 、ロボットの手先の制御とかが一ヘルツとか 数十ヘルツでちゃんときれいな動きできるかっていうと やってみと分かるんですけど、めちゃめちゃがたがたしたりだとか が大きくなるほどリアルタイム性っいうのが失われていくので ほぼ モデルの大きさのからくる汎用性を担保し つつも、どうリアルタイム性も担保するかっていうところのトレードオフを どう解決するかっていうのはかなり工夫が必要ですし、これまでの古典的なロボティクス でも このレイテンシーの話とかも議論されてき ていので、その辺の知見と組み合わせることによっていいシステム作れるはずだよね というのが、そう 近年考えられていることかなと思い 。二つ目は、結構さっきのUS中国でたくさんデータ集めるよう 期間が出てきてるっ話をしましたけど、そこからまた モデルを作れているのは、一部の研究機関というか、一部の企業に ほぼなってきていて、限られた機関だけがマルチ問題を極める 構築できる知見を持ってますし、どんどんデータとかモデルの 得点加速しているよねというような状況になってきています。 中でフィジカルの領域 ですね、のAIをどうやって作っていくのが社会としていいの かみたいなのは、かなりまだ議論の余地があるところかなというふに思っています 、三番目のところは、最後にお話ししたところですね。 。今、今日の時点で何をビジネスにもなら ない、何も一問おいしくないのに、じゃあデータを 一年間ずっと集め続けてくださいみたいなのは、あんまりビジネス の設計としてもあまりよろしくないというか、それは LGBTはあまり高くないよねっていうふには考えられますけれども、 、実際人間を今やってるビジネスの上で 人間を支援するツールとしてVLデータを取っていると 、結果的に、fiologoための学習データになてるよねみたいなロードマップを定める というのは、かなり大事ですし、最初五十パーで しか自動化できてないけれども、じゃあその残りの五十パーセントは 人間遠隔操作することで補って、じゃあその隔操の データからそれが七十五パーセントになたら、人間の労力 っ半分で済むよねみたいな、いかに 最初から完璧にVLAとか 基盤モデルを動かすんじゃなくて、いかに徐々に自動化率を高めるかってアプローチして システム全体を考えていのがいいのかなといううに思っています。最後 今回はLMの講義なんで、データとかモデルとかソフトウェアの話 してなかったんですけど、実際、こういう基盤モデルを動かす ために、どういうロボットの設計がいいとか 、こういうデータを集めるために、じゃあ営業操作は アドウェアはこういう方がいいよねとか 、モーターのレベルとからシステム設計まで LLMであるとか、ロボット基盤モデルっいうのが存在する 活用されるっいう前提で設計し直す て、互いにコミュケーションしながら開発するのが大事だよね いうところがあって、お互い理解しながら研究開発を進めるのがすごく重要な フェーズできてるんじゃなかなというふに思っています に、この業域、テニスカルAの業域 とか、ロボットかける基盤モデルみたいな領域は、かなり 今後社会で活用される 余地の大きい領域だと思いますので、もし 興味ある方がいれば、いつでもお声がけ いただければなと思いますし、質問等あれば お答えできればなと思いますので、ぜひ営業なく お声がけいただければと思います になります。ありがとうございました",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 2013.4601002384186,
      "end": 2413.7900002384185,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    },
    {
      "id": "a28e9174-8087-49e1-b159-a2629ce19b9b",
      "text": "、それではただ今から五分間の休憩に入ります。 十九時五十五分再開予定といたします。 休憩後は後半の坂本浩嗣からの フォーリパートとなります。その後演習パート六がですね ね、になります。 、それでは休憩でお願いいします",
      "speaker": "SPEAKER_0",
      "speaker_id": 0,
      "is_user": false,
      "person_id": null,
      "start": 2418.0600002384185,
      "end": 2435.030000238419,
      "translations": [],
      "speech_profile_processed": true,
      "stt_provider": null
    }
  ],
  "transcript_segments_compressed": true,
  "geolocation": null,
  "photos": [],
  "audio_files": [
    {
      "id": "0e341fe6-15c3-45eb-b27e-0fd62851ef13",
      "uid": "ZYG1703CexOSRPkddtI4dTcdS3s1",
      "conversation_id": "b95ec851-21c0-4273-a739-b74a88075401",
      "chunk_timestamps": [
        1768990201.309,
        1768990206.795,
        1768990211.771,
        1768990216.79,
        1768990221.891,
        1768990226.864,
        1768990232.766,
        1768990237.775,
        1768990242.801,
        1768990247.749,
        1768990252.792,
        1768990258.348,
        1768990262.781,
        1768990267.764,
        1768990272.796,
        1768990277.79,
        1768990282.856,
        1768990287.767,
        1768990292.787,
        1768990297.787,
        1768990302.773,
        1768990307.757,
        1768990312.791,
        1768990317.821,
        1768990322.785,
        1768990327.767,
        1768990332.795,
        1768990337.79,
        1768990342.845,
        1768990347.763,
        1768990352.79,
        1768990357.791,
        1768990362.781,
        1768990367.772,
        1768990372.778,
        1768990377.789,
        1768990382.781,
        1768990387.781,
        1768990392.847,
        1768990397.785,
        1768990402.789,
        1768990407.772,
        1768990412.785,
        1768990417.788,
        1768990422.784,
        1768990428.798,
        1768990433.776,
        1768990438.785,
        1768990443.787,
        1768990448.798,
        1768990453.823,
        1768990458.784,
        1768990464.025,
        1768990468.786,
        1768990473.783,
        1768990478.787,
        1768990483.764,
        1768990488.786,
        1768990493.795,
        1768990499.665,
        1768990504.783,
        1768990509.779,
        1768990514.786,
        1768990519.77,
        1768990524.819,
        1768990529.855,
        1768990534.8,
        1768990539.766,
        1768990544.793,
        1768990549.788,
        1768990554.78,
        1768990559.748,
        1768990564.8,
        1768990571.773,
        1768990576.869,
        1768990582.796,
        1768990587.793,
        1768990592.83,
        1768990597.782,
        1768990602.792,
        1768990607.76,
        1768990615.289,
        1768990619.768,
        1768990624.809,
        1768990629.788,
        1768990634.786,
        1768990639.754,
        1768990644.792,
        1768990649.845,
        1768990654.793,
        1768990659.791,
        1768990664.849,
        1768990670.002,
        1768990675.844,
        1768990680.834,
        1768990685.778,
        1768990691.774,
        1768990696.795,
        1768990701.789,
        1768990706.784,
        1768990711.768,
        1768990716.792,
        1768990721.786,
        1768990726.808,
        1768990731.767,
        1768990739.37,
        1768990744.772,
        1768990750.787,
        1768990755.771,
        1768990760.788,
        1768990765.788,
        1768990770.788,
        1768990775.765,
        1768990780.791,
        1768990785.868,
        1768990790.788,
        1768990795.767,
        1768990800.789,
        1768990805.782,
        1768990810.799,
        1768990815.779,
        1768990820.787,
        1768990825.789,
        1768990830.78,
        1768990835.761,
        1768990840.786,
        1768990845.786,
        1768990850.789,
        1768990855.772,
        1768990860.77,
        1768990865.767,
        1768990870.792,
        1768990875.773,
        1768990880.79,
        1768990885.92,
        1768990890.83,
        1768990895.77,
        1768990900.795,
        1768990905.784,
        1768990910.782,
        1768990915.761,
        1768990920.795,
        1768990926.793,
        1768990931.764,
        1768990936.787,
        1768990941.799,
        1768990946.784,
        1768990951.764,
        1768990956.793,
        1768990961.787,
        1768990966.79,
        1768990972.282,
        1768990977.782,
        1768990982.787,
        1768990987.767,
        1768990992.783,
        1768990998.787,
        1768991003.782,
        1768991008.792,
        1768991013.796,
        1768991018.786,
        1768991023.783,
        1768991028.815,
        1768991033.787,
        1768991038.787,
        1768991043.788,
        1768991048.791,
        1768991053.801,
        1768991058.795,
        1768991064.106,
        1768991069.787,
        1768991074.805,
        1768991079.769,
        1768991084.788,
        1768991089.786,
        1768991094.792,
        1768991102.795,
        1768991107.79,
        1768991112.917,
        1768991117.799,
        1768991122.807,
        1768991127.782,
        1768991132.862,
        1768991137.95,
        1768991143.035,
        1768991147.768,
        1768991152.79,
        1768991158.534,
        1768991163.779,
        1768991168.812,
        1768991174.64,
        1768991178.882,
        1768991183.773,
        1768991188.796,
        1768991193.793,
        1768991198.794,
        1768991204.792,
        1768991209.788,
        1768991215.256,
        1768991219.775,
        1768991225.787,
        1768991233.794,
        1768991238.795,
        1768991243.789,
        1768991248.793,
        1768991253.805,
        1768991258.79,
        1768991264.187
      ],
      "provider": "gcp",
      "started_at": "2026-01-21T10:10:01.309000+00:00",
      "duration": 1067.878
    }
  ],
  "private_cloud_sync_enabled": true,
  "apps_results": [
    {
      "app_id": "01K89EAAY3XMJA0SJ7NSP6N5GT",
      "content": "## 基盤モデルとロボットの相性（〜2022年頃までの背景）\n- 単一モデルで多数タスク対応、プロンプトで切替  \n- データ・計算スケールで汎化性能向上  \n- 言語・画像・点群・音声などマルチモーダル統合容易  \n\n## LLM/VLMのロボット応用の初期（2022〜2023前半）\n- 自然言語→API組合せでロボット動作コード自動生成  \n- LLMで報酬関数生成し強化学習に利用  \n- CLIP等で地図に意味特徴付与し「テキストで場所指定」実現  \n\n## ロボット基盤モデルとデータ集約（2023年以降）\n- RT-1, RT-2：画像＋テキスト→関節角など行動を直接出力  \n- Googleオフィスで約23台を1.5年遠隔操作しデータ収集  \n- Open X-Embodimentで世界中のロボットデータを共通形式で統合  \n\n## データ収集手法の多様化（リアル＋シミュレーション）\n- Aloha系、ゼロ等の遠隔操作リグで安価に実演データ収集  \n- UMIグローブ＋GoProで人の手操作を簡便に計測  \n- NVIDIA Isaac/Omniverseで大量・高精細シミュレーションデータ生成  \n\n## 学習アーキテクチャとモダリティ統合\n- Transformerで言語・音声・ロボット行動など共通処理  \n- 行動直接予測、Q値学習、世界モデル型など統一枠組み化  \n- Diffusion policy等、拡散モデルで柔軟なロボット動作生成  \n\n## フィジカルAIと産業・スタートアップ動向\n- 上海等でヒューマノイド大量遠隔操作センター構築  \n- 1X, Figure AIなどが家庭・工場向けヒューマノイド開発  \n- 単価数千万円→数百万円規模へ低下し量産・スケール進行  \n\n## ロボット学習用データの三層モデル\n- リアルロボット：教師信号強いが高コスト・量が限られる  \n- 人工（シム・拡張）：コスト中程度で多様性を補完  \n- ウェブ（YouTube等）：超大量・汎用だが制御信号弱い  \n\n## 人間データ活用の新しい方向性\n- 人間＝世界最多ロボットと捉え行動データ大量取得  \n- 一人称視点＋VLMで手順書自動生成し人支援と両立  \n- 人支援ツールとしての価値と将来ロボット学習を両立設計  \n\n## 課題と今後の設計思想\n- 大規模モデルは推論遅く、リアルタイム制御とトレードオフ  \n- データ・モデル構築が一部大企業に集中し格差拡大懸念  \n- 人間遠隔操作と自動化を組み合わせ段階的に自動化率向上  \n\n## Next steps\n- LLM/VLM前提でロボットハード・システム設計を再検討  \n- リアル・人工・ウェブデータを組み合わせた収集戦略立案  \n- 人間支援ビジネスとロボット自動化を両立するプロジェクト検討"
    }
  ],
  "suggested_summarization_apps": [
    "01K89EAAY3XMJA0SJ7NSP6N5GT"
  ],
  "plugins_results": [],
  "external_data": null,
  "app_id": null,
  "discarded": false,
  "visibility": "private",
  "starred": false,
  "processing_memory_id": null,
  "processing_conversation_id": null,
  "status": "completed",
  "is_locked": false,
  "data_protection_level": "standard",
  "folder_id": "f5c473b3-cda8-4a2b-851b-264cc4b775e9"
}